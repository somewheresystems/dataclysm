{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get most recent arXiv manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://arxiv-dataset/metadata-v5/arxiv-metadata-oai.json...\n",
      "If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o \"GSUtil:parallel_process_count=1\"`. Note that multithreading is still available even if you disable multiprocessing.\n",
      "\n",
      "| [1 files][  4.2 GiB/  4.2 GiB]   25.8 MiB/s                                   \n",
      "Operation completed over 1 objects/4.2 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://arxiv-dataset/metadata-v5/arxiv-metadata-oai.json .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Get all the wikipedia parquet files from the specified directory\n",
    "parquet_files = glob.glob('/Users/s2/Library/Mobile Documents/com~apple~CloudDocs/Datasets/dataclysm/wikipedia-titles/parquet/*.parquet')\n",
    "\n",
    "for file in parquet_files:\n",
    "    # Check if 'large' is in the filename\n",
    "    if 'large' in file:\n",
    "        # Replace 'large' with 'small' in the filename\n",
    "        new_file = file.replace('large', 'small')\n",
    "        # Rename the file\n",
    "        os.rename(file, new_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings based on arXiv data manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import FlagModel\n",
    "from datasets import load_dataset\n",
    "import psutil\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = FlagModel('BAAI/bge-small-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"Write a representation of the following title which is optimized for retrieval:\",\n",
    "                  use_fp16=True)\n",
    "\n",
    "# Define the directory\n",
    "directory = '/Users/s2/Repos/harness/output'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"arxiv-metadata-oai.jsonl\", split=\"train\", cache_dir=directory)\n",
    "\n",
    "# Define the process_data function\n",
    "def process_data(data):\n",
    "    # Remove delimiters\n",
    "    title = data['title'].replace('\\n', ' ')\n",
    "    abstract = data['abstract'].replace('\\n', ' ')\n",
    "    \n",
    "    # Embed the 'title' and 'abstract' fields\n",
    "    title_embedding = model.encode([title])\n",
    "    abstract_embedding = model.encode([abstract])\n",
    "    \n",
    "    # Add the embeddings to the data\n",
    "    data['title_embedding'] = title_embedding.tolist()\n",
    "    data['abstract_embedding'] = abstract_embedding.tolist()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 100000\n",
    "batch_data = []\n",
    "batch_index = 0\n",
    "\n",
    "# Load the last processed index from a file if it exists\n",
    "last_processed_index_file = os.path.join(directory, \"last_processed_index.txt\")\n",
    "if os.path.isfile(last_processed_index_file):\n",
    "    with open(last_processed_index_file, \"r\") as file:\n",
    "        batch_index = int(file.read())\n",
    "\n",
    "# Process the dataset\n",
    "for i in tqdm(range(batch_index, len(dataset)), desc=\"Processing dataset\"):\n",
    "    data = dataset[i]\n",
    "    processed_data = process_data(data)\n",
    "    batch_data.append(processed_data)\n",
    "    \n",
    "    # If batch size is reached, write to file and reset batch data\n",
    "    if len(batch_data) == batch_size:\n",
    "        file_path = os.path.join(directory, f\"{batch_index}_arxiv_metadata_oai.jsonl\")\n",
    "        with jsonlines.open(file_path, mode='w') as writer:\n",
    "            for item in batch_data:\n",
    "                writer.write(item)\n",
    "        batch_data = []\n",
    "        batch_index += 1\n",
    "        # Save the last processed index to a file\n",
    "        with open(last_processed_index_file, \"w\") as file:\n",
    "            file.write(str(batch_index))\n",
    "\n",
    "# Write remaining data to file\n",
    "if batch_data:\n",
    "    file_path = os.path.join(directory, f\"{batch_index}_arxiv_metadata_oai.jsonl\")\n",
    "    with jsonlines.open(file_path, mode='w') as writer:\n",
    "        for item in batch_data:\n",
    "            writer.write(item)\n",
    "    batch_data = []\n",
    "    # Save the last processed index to a file\n",
    "    with open(last_processed_index_file, \"w\") as file:\n",
    "        file.write(str(batch_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting files: 100%|██████████| 65/65 [05:43<00:00,  5.28s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import FlagModel\n",
    "from datasets import load_dataset\n",
    "import psutil\n",
    "import pandas as pd\n",
    "\n",
    "directory = '/Users/s2/Library/Mobile Documents/com~apple~CloudDocs/Datasets/dataclysm/wikipedia-titles-lite'\n",
    "# Function to convert JSONL to Parquet\n",
    "def convert_jsonl_to_parquet(jsonl_file_path, parquet_file_path):\n",
    "    # Read the JSONL file into a pandas DataFrame\n",
    "    df = pd.read_json(jsonl_file_path, lines=True)\n",
    "    \n",
    "    # Write the DataFrame to a Parquet file\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "# Convert all JSONL files in the directory to Parquet\n",
    "for file_name in tqdm(os.listdir(directory), desc=\"Converting files\"):\n",
    "    if file_name.endswith(\".jsonl\"):\n",
    "        jsonl_file_path = os.path.join(directory, file_name)\n",
    "        parquet_file_path = os.path.join(directory, file_name.replace(\".jsonl\", \".parquet\"))\n",
    "        convert_jsonl_to_parquet(jsonl_file_path, parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct wrong ID type in Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 34/34 [01:22<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "directory = \"../output/parquet\"\n",
    "\n",
    "# Function to modify the first column type of a Parquet file\n",
    "def modify_first_column_type(parquet_file_path):\n",
    "    # Read the Parquet file into a pandas DataFrame\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    \n",
    "    # Convert the first column to string type\n",
    "    df[df.columns[0]] = df[df.columns[0]].astype(str)\n",
    "    \n",
    "    # Write the DataFrame back to the Parquet file\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "# Iterate through all the Parquet files in the directory\n",
    "for file_name in tqdm(os.listdir(directory), desc=\"Processing files\"):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        # Get the full file path\n",
    "        parquet_file_path = os.path.join(directory, file_name)\n",
    "        # Modify the first column type of the Parquet file\n",
    "        modify_first_column_type(parquet_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct model name on Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab a random Dataclysm arXiv paper's PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%pip install wget\n",
    "import wget\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import FlagModel\n",
    "from datasets import load_dataset\n",
    "import psutil\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the huggingface dataset\n",
    "dataset = load_dataset('somewheresystems/dataclysm-arxiv')\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Grab a random entry from the DataFrame\n",
    "random_entry = df.sample()\n",
    "\n",
    "# Get the ID of the random entry\n",
    "id = random_entry['id'].values[0]\n",
    "\n",
    "# Construct the URL\n",
    "url = f\"https://arxiv.org/pdf/{id}.pdf\"\n",
    "\n",
    "# Download the PDF using wget\n",
    "wget.download(url, out=os.path.join(directory, f\"{id}.pdf\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: FlagEmbedding in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (1.1.8)\n",
      "Requirement already satisfied: datasets in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from FlagEmbedding) (2.1.2)\n",
      "Collecting transformers==4.34.0 (from FlagEmbedding)\n",
      "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: accelerate>=0.20.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from FlagEmbedding) (0.26.1)\n",
      "Requirement already satisfied: sentence-transformers in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from FlagEmbedding) (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0->FlagEmbedding)\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: psutil in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (5.9.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0->FlagEmbedding) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->FlagEmbedding) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->FlagEmbedding) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->FlagEmbedding) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->FlagEmbedding) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: torchvision in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (0.16.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (1.11.4)\n",
      "Requirement already satisfied: nltk in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (0.1.99)\n",
      "Requirement already satisfied: six>=1.5 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tokenizers-0.14.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting sentence-transformers (from FlagEmbedding)\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "  Using cached sentence-transformers-2.2.1.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is still looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached sentence-transformers-2.0.0.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached sentence-transformers-1.2.1.tar.gz (80 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached sentence-transformers-1.2.0.tar.gz (81 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate>=0.20.1 (from FlagEmbedding)\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0->FlagEmbedding)\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.3)\n",
      "Requirement already satisfied: click in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from nltk->sentence-transformers->FlagEmbedding) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from nltk->sentence-transformers->FlagEmbedding) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->FlagEmbedding) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from torchvision->sentence-transformers->FlagEmbedding) (10.2.0)\n",
      "Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "Using cached datasets-2.14.7-py3-none-any.whl (520 kB)\n",
      "Using cached tokenizers-0.14.1-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.2\n",
      "    Uninstalling huggingface-hub-0.20.2:\n",
      "      Successfully uninstalled huggingface-hub-0.20.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.16.1\n",
      "    Uninstalling datasets-2.16.1:\n",
      "      Successfully uninstalled datasets-2.16.1\n",
      "Successfully installed datasets-2.14.7 huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c pytorch faiss-cpu transformers datasets jsonlines\n",
    "%pip install -U FlagEmbedding datasets\n",
    "%pip install llama-cpp-python httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize arXiv Abstract + Title Indices\n",
    "This process takes ~15 minutes to index (M3 Max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Wikipedia Database + Index\n",
    "This process takes 2x as much time as arXiv to download, about ~12 minutes to index (M3 Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Current memory usage: 35964.890625 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 65/65 [00:00<00:00, 113.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage: 36104.484375 MB\n",
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text', 'title_embedding'],\n",
      "    num_rows: 6458670\n",
      "})\n",
      "['id', 'url', 'title', 'text', 'title_embedding']\n",
      "{'id': Value(dtype='int64', id=None), 'url': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'title_embedding': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None)}\n",
      "Current memory usage: 36104.484375 MB\n",
      "Flattening title_embedding and adding FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6458670/6458670 [12:42<00:00, 8470.21 examples/s]\n",
      "100%|██████████| 6459/6459 [00:39<00:00, 165.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index for title_embedding added.\n",
      "Current memory usage: 77425.046875 MB\n",
      "Current memory usage: 77425.234375 MB\n",
      "Datasets loaded.\n",
      "Initializing model...\n",
      "Model initialized.\n",
      "Current memory usage: 75355.0 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import FlagModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage():\n",
    "    print(f\"Current memory usage: {psutil.Process().memory_info().rss / 1024 ** 2} MB\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "print_memory_usage()\n",
    "dataclysm_wikipedia = load_dataset('somewheresystems/dataclysm-wikipedia', split=\"train\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Check the structure of the dataset, particularly the 'title_embedding' and 'abstract_embedding' columns\n",
    "print(dataclysm_wikipedia)\n",
    "print(dataclysm_wikipedia.column_names)\n",
    "print(dataclysm_wikipedia.features)\n",
    "print_memory_usage()\n",
    "\n",
    "# Define a function to flatten the embeddings and add FAISS index\n",
    "def flatten_and_add_faiss_index(dataset, column_name):\n",
    "    embedding_shape = np.array(dataset[0][column_name]).shape\n",
    "    if len(embedding_shape) == 2:\n",
    "        print(f\"Flattening {column_name} and adding FAISS index...\")\n",
    "        # Flatten the column before adding the FAISS index\n",
    "        dataset = dataset.map(lambda x: {column_name: np.concatenate(x[column_name])})\n",
    "        dataset = dataset.add_faiss_index(column=column_name)\n",
    "        print(f\"FAISS index for {column_name} added.\")\n",
    "    else:\n",
    "        print(f\"Cannot add FAISS index for {column_name}.\")\n",
    "    print_memory_usage()\n",
    "    return dataset\n",
    "\n",
    "# Add FAISS indices for 'title_embedding' and 'abstract_embedding' and save them to different datasets\n",
    "dataclysm_wikipedia_indexed = flatten_and_add_faiss_index(dataclysm_wikipedia, 'title_embedding')\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "# Define the model\n",
    "print(\"Initializing model...\")\n",
    "model = FlagModel('BAAI/bge-small-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"Write a representation of the following query which is optimized for using a similarity search for retrieval:\",\n",
    "                  use_fp16=True)\n",
    "print(\"Model initialized.\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize arXiv Abstract + Title Indices\n",
    "This process takes ~15 minutes to index (M3 Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Current memory usage: 71617.296875 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 34/34 [00:00<00:00, 127.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage: 71782.09375 MB\n",
      "Dataset({\n",
      "    features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'abstract', 'report-no', 'categories', 'versions', 'title_embedding', 'abstract_embedding'],\n",
      "    num_rows: 3360984\n",
      "})\n",
      "['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'abstract', 'report-no', 'categories', 'versions', 'title_embedding', 'abstract_embedding']\n",
      "{'id': Value(dtype='string', id=None), 'submitter': Value(dtype='string', id=None), 'authors': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'comments': Value(dtype='string', id=None), 'journal-ref': Value(dtype='string', id=None), 'doi': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None), 'report-no': Value(dtype='string', id=None), 'categories': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'versions': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'title_embedding': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None), 'abstract_embedding': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None)}\n",
      "Current memory usage: 71782.109375 MB\n",
      "Flattening title_embedding and adding FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3361/3361 [00:19<00:00, 172.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index for title_embedding added.\n",
      "Current memory usage: 81655.0 MB\n",
      "Flattening abstract_embedding and adding FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3360984/3360984 [08:51<00:00, 6318.74 examples/s]\n",
      "100%|██████████| 3361/3361 [00:20<00:00, 166.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index for abstract_embedding added.\n",
      "Current memory usage: 65721.453125 MB\n",
      "Current memory usage: 65720.3125 MB\n",
      "Datasets loaded.\n",
      "Initializing model...\n",
      "Model initialized.\n",
      "Current memory usage: 64288.28125 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import FlagModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage():\n",
    "    print(f\"Current memory usage: {psutil.Process().memory_info().rss / 1024 ** 2} MB\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "print_memory_usage()\n",
    "dataclysm_arxiv = load_dataset('somewheresystems/dataclysm-arxiv', split=\"train\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Check the structure of the dataset, particularly the 'title_embedding' and 'abstract_embedding' columns\n",
    "print(dataclysm_arxiv)\n",
    "print(dataclysm_arxiv.column_names)\n",
    "print(dataclysm_arxiv.features)\n",
    "print_memory_usage()\n",
    "\n",
    "# Define a function to flatten the embeddings and add FAISS index\n",
    "def flatten_and_add_faiss_index(dataset, column_name):\n",
    "    embedding_shape = np.array(dataset[0][column_name]).shape\n",
    "    if len(embedding_shape) == 2:\n",
    "        print(f\"Flattening {column_name} and adding FAISS index...\")\n",
    "        # Flatten the column before adding the FAISS index\n",
    "        dataset = dataset.map(lambda x: {column_name: np.concatenate(x[column_name])})\n",
    "        dataset = dataset.add_faiss_index(column=column_name)\n",
    "        print(f\"FAISS index for {column_name} added.\")\n",
    "    else:\n",
    "        print(f\"Cannot add FAISS index for {column_name}.\")\n",
    "    print_memory_usage()\n",
    "    return dataset\n",
    "\n",
    "# Add FAISS indices for 'title_embedding' and 'abstract_embedding' and save them to different datasets\n",
    "dataclysm_title_indexed = flatten_and_add_faiss_index(dataclysm_arxiv, 'title_embedding')\n",
    "dataclysm_abstract_indexed = flatten_and_add_faiss_index(dataclysm_arxiv, 'abstract_embedding')\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "# Define the model\n",
    "print(\"Initializing model...\")\n",
    "model = FlagModel('BAAI/bge-small-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"Write a representation of the following query which is optimized for using a similarity search for retrieval:\",\n",
    "                  use_fp16=True)\n",
    "print(\"Model initialized.\")\n",
    "print_memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  arXiv Composite Search with regex Rerank\n",
    "Search by both Abstract and Title similarity, rank both descending by score. \n",
    "1. If a duplicate (title and abstract hit) is found, it increases the score by a factor of 2. \n",
    "2. If regex finds the query in the abstract, it increases the score by 0.1 (additive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding query...\n",
      "Query encoded.\n",
      "Retrieving examples by abstract similarity...\n",
      "Examples retrieved.\n",
      "Retrieving examples by title similarity...\n",
      "Examples retrieved.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "QUERY: **Attention Is All You Need**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>versions</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>source</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>math/9409228</td>\n",
       "      <td>None</td>\n",
       "      <td>Alphonse P. Magnus</td>\n",
       "      <td>Painlev\\'e equations for semi-classical recurrence coefficients</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td><details><summary>Abstract</summary>  The title says it all.\\n</details></td>\n",
       "      <td>OP-SF 6 Sep 1994</td>\n",
       "      <td>[math.CA]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/math/9409228\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>math/9803101</td>\n",
       "      <td>Robion C. Kirby</td>\n",
       "      <td>Robion C. Kirby and Laurence R. Taylor</td>\n",
       "      <td>A survey of 4-manifolds through the eyes of surgery</td>\n",
       "      <td>25 pages. To appear in Wall's 60th birthday volume</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td><details><summary>Abstract</summary>  The title says it all.\\n</details></td>\n",
       "      <td>None</td>\n",
       "      <td>[math.GT]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/math/9803101\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1702.04226</td>\n",
       "      <td>Wenyun Ju</td>\n",
       "      <td>Wenyun Ju</td>\n",
       "      <td>Cascading Outage Simulation Based on Dynamic Fast Decoupled Load Flow\\n  Model</td>\n",
       "      <td>There is an error in equation (23)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td><details><summary>Abstract</summary>  Frequency is an important\\n</details></td>\n",
       "      <td>None</td>\n",
       "      <td>[cs.SY]</td>\n",
       "      <td>[v1, v2, v3]</td>\n",
       "      <td>0.979529</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1702.04226\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>math/9803061</td>\n",
       "      <td>Hans Schneider</td>\n",
       "      <td>Hans Schneider (U Wisconsin - Madison)</td>\n",
       "      <td>Some personal reminiscences of Olga Taussky</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td><details><summary>Abstract</summary>  The title says it all\\n</details></td>\n",
       "      <td>None</td>\n",
       "      <td>[math.HO math.RA]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>0.959123</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/math/9803061\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1806.06771</td>\n",
       "      <td>Christoph Anderson</td>\n",
       "      <td>Christoph Anderson, Isabel H\\\"ubener, Ann-Kathrin Seipp, Sandra Ohly,\\n  Klaus David, Veljko Pejovic</td>\n",
       "      <td>A Survey of Attention Management Systems in Ubiquitous Computing\\n  Environments</td>\n",
       "      <td>27 pages, 7 figures</td>\n",
       "      <td>Proceedings of the ACM on Interactive, Mobile, Wearable and\\n  Ubiquitous Technologies, vol. 2, no. 2, pp. 58:1-58:27, June 2018</td>\n",
       "      <td>10.1145/3214261</td>\n",
       "      <td><details><summary>Abstract</summary>  Today's information and communication devices provide always-on connectivity,\\ninstant access to an endless repository of information, and represent the most\\ndirect point of contact to almost any person in the world. Despite these\\nadvantages, devices such as smartphones or personal computers lead to the\\nphenomenon of attention fragmentation, continuously interrupting individuals'\\nactivities and tasks with notifications. Attention management systems aim to\\nprovide active support in such scenarios, managing interruptions, for example,\\nby postponing notifications to opportune moments for information delivery. In\\nthis article, we review attention management system research with a particular\\nfocus on ubiquitous computing environments. We first examine cognitive theories\\nof attention and extract guidelines for practical attention management systems.\\nMathematical models of human attention are at the core of these systems, and in\\nthis article, we review sensing and machine learning techniques that make such\\nmodels possible. We then discuss design challenges towards the implementation\\nof such systems, and finally, we investigate future directions in this area,\\npaving the way for new approaches and systems supporting users in their\\nattention management.\\n</details></td>\n",
       "      <td>None</td>\n",
       "      <td>[cs.HC]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>0.956954</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1806.06771\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1710.03743</td>\n",
       "      <td>Mat\\=iss Rikters</td>\n",
       "      <td>Mat\\=iss Rikters, Mark Fishel</td>\n",
       "      <td>Confidence through Attention</td>\n",
       "      <td>None</td>\n",
       "      <td>Machine Translation Summit XVI, Nagoya, Japan, September 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  Attention distributions of the generated translations are a useful bi-product\\nof attention-based recurrent neural network translation models and can be\\ntreated as soft alignments between the input and output tokens. In this work,\\nwe use attention distributions as a confidence metric for output translations.\\nWe present two strategies of using the attention distributions: filtering out\\nbad translations from a large back-translated corpus, and selecting the best\\ntranslation in a hybrid setup of two different translation systems. While\\nmanual evaluation indicated only a weak correlation between our confidence\\nscore and human judgments, the use-cases showed improvements of up to 2.22 BLEU\\npoints for filtering and 0.99 points for hybrid translation, tested on\\nEnglish<->German and English<->Latvian translation.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>0.735877</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1710.03743\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1810.10126</td>\n",
       "      <td>Yang Li</td>\n",
       "      <td>Yang Li, Lukasz Kaiser, Samy Bengio, Si Si</td>\n",
       "      <td>Area Attention</td>\n",
       "      <td>8 pages plus references</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  Existing attention mechanisms are trained to attend to individual items in a\\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\\ntoken or an image grid. We propose area attention: a way to attend to areas in\\nthe memory, where each area contains a group of items that are structurally\\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\\n1D memory such as natural language sentences. Importantly, the shape and the\\nsize of an area are dynamically determined via learning, which enables a model\\nto attend to information with varying granularity. Area attention can easily\\nwork with existing model architectures such as multi-head attention for\\nsimultaneously attending to multiple areas in the memory. We evaluate area\\nattention on two tasks: neural machine translation (both character and\\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\\nbaselines in all the cases. These improvements are obtainable with a basic form\\nof area attention that is parameter free.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.LG cs.AI cs.CL stat.ML]</td>\n",
       "      <td>[v1, v2, v3, v4, v5, v6]</td>\n",
       "      <td>0.710768</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1810.10126\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1810.13409</td>\n",
       "      <td>Ofir Press</td>\n",
       "      <td>Ofir Press, Noah A. Smith</td>\n",
       "      <td>You May Not Need Attention</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  In NMT, how far can we get without attention and without separate encoding\\nand decoding? To answer that question, we introduce a recurrent neural\\ntranslation model that does not use attention and does not have a separate\\nencoder and decoder. Our eager translation model is low-latency, writing target\\ntokens as soon as it reads the first source token, and uses constant memory\\nduring decoding. It performs on par with the standard attention-based model of\\nBahdanau et al. (2014), and better on long sentences.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>0.582988</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1810.13409\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1804.02391</td>\n",
       "      <td>Saumya Jetley</td>\n",
       "      <td>Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H.S. Torr</td>\n",
       "      <td>Learn To Pay Attention</td>\n",
       "      <td>International Conference on Learning Representations 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  We propose an end-to-end-trainable attention module for convolutional neural\\nnetwork (CNN) architectures built for image classification. The module takes as\\ninput the 2D feature vector maps which form the intermediate representations of\\nthe input image at different stages in the CNN pipeline, and outputs a 2D\\nmatrix of scores for each map. Standard CNN architectures are modified through\\nthe incorporation of this module, and trained under the constraint that a\\nconvex combination of the intermediate 2D feature vectors, as parameterised by\\nthe score matrices, must \\textit{alone} be used for classification.\\nIncentivised to amplify the relevant and suppress the irrelevant or misleading,\\nthe scores thus assume the role of attention values. Our experimental\\nobservations provide clear evidence to this effect: the learned attention maps\\nneatly highlight the regions of interest while suppressing background clutter.\\nConsequently, the proposed function is able to bootstrap standard CNN\\narchitectures for the task of image classification, demonstrating superior\\ngeneralisation over 6 unseen benchmark datasets. When binarised, our attention\\nmaps outperform other CNN-based attention maps, traditional saliency maps, and\\ntop object proposals for weakly supervised segmentation as demonstrated on the\\nObject Discovery dataset. We also demonstrate improved robustness against the\\nfast gradient sign method of adversarial attack.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CV cs.AI]</td>\n",
       "      <td>[v1, v2]</td>\n",
       "      <td>0.580535</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1804.02391\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1706.03762</td>\n",
       "      <td>Ashish Vaswani</td>\n",
       "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\n  Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>15 pages, 5 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL cs.LG]</td>\n",
       "      <td>[v1, v2, v3, v4, v5]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1706.03762\">Link</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Attention Is All You Need\"\n",
    "print(\"Encoding query...\")\n",
    "query_embedding = model.encode([query])\n",
    "print(\"Query encoded.\")\n",
    "\n",
    "print(\"Retrieving examples by abstract similarity...\")\n",
    "scores_abstract, retrieved_examples_abstract = dataclysm_abstract_indexed.get_nearest_examples('abstract_embedding', query_embedding, k=10)\n",
    "print(\"Examples retrieved.\")\n",
    "\n",
    "print(\"Retrieving examples by title similarity...\")\n",
    "scores_title, retrieved_examples_title = dataclysm_title_indexed.get_nearest_examples('title_embedding', query_embedding, k=10)\n",
    "print(\"Examples retrieved.\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Convert retrieved examples to DataFrame\n",
    "df_abstract = pd.DataFrame(retrieved_examples_abstract)\n",
    "df_title = pd.DataFrame(retrieved_examples_title)\n",
    "\n",
    "# Calculate similarity score in percentage\n",
    "df_abstract['similarity_score'] = scores_abstract\n",
    "df_title['similarity_score'] = scores_title\n",
    "\n",
    "# Add a column to denote the source of retrieval\n",
    "df_abstract['source'] = 'A'\n",
    "df_title['source'] = 'T'\n",
    "\n",
    "# Drop 'title_embedding' and 'abstract_embedding' columns\n",
    "df_abstract = df_abstract.drop(columns=['title_embedding', 'abstract_embedding'])\n",
    "df_title = df_title.drop(columns=['title_embedding', 'abstract_embedding'])\n",
    "\n",
    "# Drop empty columns\n",
    "df_abstract = df_abstract.dropna(axis=1, how='all')\n",
    "df_title = df_title.dropna(axis=1, how='all')\n",
    "\n",
    "# Create a \"click to expand\" for the abstract so it doesn't take up much space\n",
    "df_abstract['abstract'] = df_abstract['abstract'].apply(lambda x: f'<details><summary>Abstract</summary>{x}</details>')\n",
    "df_title['abstract'] = df_title['abstract'].apply(lambda x: f'<details><summary>Abstract</summary>{x}</details>')\n",
    "\n",
    "# Create a URL field with a hyperlink which is constructed by appending the id onto the end of arxiv.org/abs/\n",
    "df_abstract['URL'] = df_abstract['id'].apply(lambda x: f'<a href=\"https://arxiv.org/abs/{x}\">Link</a>')\n",
    "df_title['URL'] = df_title['id'].apply(lambda x: f'<a href=\"https://arxiv.org/abs/{x}\">Link</a>')\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df = pd.concat([df_abstract, df_title])\n",
    "\n",
    "# Normalize the similarity score to be between 0 and 1\n",
    "df['similarity_score'] = df['similarity_score'] / df['similarity_score'].max()\n",
    "\n",
    "# Increase the score if the query is found in the abstract\n",
    "df['similarity_score'] = df.apply(lambda row: row['similarity_score'] + 0.1 if re.search(query, row['abstract'], re.IGNORECASE) else row['similarity_score'], axis=1)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['id'])\n",
    "\n",
    "# Sort by ascending similarity score\n",
    "df = df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f'QUERY: **{query}**'))\n",
    "display(HTML(df.to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia simple search (Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Retrieval Augmented Generation\"\n",
    "print(\"Encoding query...\")\n",
    "query_embedding = model.encode([query])\n",
    "print(\"Query encoded.\")\n",
    "\n",
    "print(\"Retrieving examples by title similarity...\")\n",
    "scores, retrieved_examples = dataclysm_wikipedia_indexed.get_nearest_examples('title_embedding', query_embedding, k=10)\n",
    "print(\"Examples retrieved.\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Convert retrieved examples to DataFrame\n",
    "df = pd.DataFrame(retrieved_examples)\n",
    "\n",
    "# Calculate similarity score in percentage\n",
    "df['similarity_score'] = scores\n",
    "\n",
    "\n",
    "# Drop 'title_embedding' and 'abstract_embedding' columns\n",
    "df = df.drop(columns=['title_embedding'])\n",
    "\n",
    "# Drop empty columns\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Create a \"click to expand\" for the abstract so it doesn't take up much space\n",
    "df['text'] = df['text'].apply(lambda x: f'<details><summary>Article Text</summary>{x}</details>')\n",
    "\n",
    "\n",
    "# Create a URL field with a hyperlink \n",
    "df['url'] = df['url'].apply(lambda x: f'<a href=\"{url}\">Link</a>')\n",
    "\n",
    "# Sort by ascending similarity score\n",
    "df = df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f'QUERY: **{query}**'))\n",
    "display(HTML(df.to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download OpenHermes-2.5-Mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-cli in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf to /Users/s2/.cache/huggingface/hub/tmpdpa1h_vs\n",
      "openhermes-2.5-mistral-7b.Q4_K_M.gguf: 100%|█| 4.37G/4.37G [02:37<00:00, 27.8MB/\n",
      "./openhermes-2.5-mistral-7b.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface-cli\n",
    "!huggingface-cli download TheBloke/OpenHermes-2.5-Mistral-7B-GGUF openhermes-2.5-mistral-7b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from openhermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = teknium_openhermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1012.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   545.82 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is the title of a paper published in 2017 that introduced a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The paper achieved significant improvements in machine translation tasks and established a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6858.21 ms\n",
      "llama_print_timings:      sample time =       9.76 ms /    94 runs   (    0.10 ms per token,  9628.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   37015.65 ms /  2298 tokens (   16.11 ms per token,    62.08 tokens per second)\n",
      "llama_print_timings:        eval time =    5096.66 ms /    93 runs   (   54.80 ms per token,    18.25 tokens per second)\n",
      "llama_print_timings:       total time =   42232.10 ms /  2391 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp import LlamaGrammar\n",
    "import pandas as pd\n",
    "import json\n",
    "import httpx\n",
    "\n",
    "model = \"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\n",
    "prompt = f\"{df[['id', 'title', 'abstract']].to_html(escape=False)} ### Instruction: Use the information above to answer the query: EXPLAIN {query} ### Response:\"\n",
    "\n",
    "\n",
    "llm = Llama(model_path=model, n_ctx=8096, last_n_tokens_size=256, n_threads=4, n_gpu_layers=0)\n",
    "\n",
    "stream = llm.create_completion(prompt, stream=True, repeat_penalty=1.1, max_tokens=256, stop=[\"\\n\"], echo=False, temperature=0, mirostat_mode = 2, mirostat_tau=4.0, mirostat_eta=1.1)\n",
    "result = \"\"\n",
    "for output in stream:\n",
    "    result += output['choices'][0]['text']\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerank results using an LLM (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= arr \n",
      "arr ::= [[] [<U+000A>] ws arr_12 []] \n",
      "value ::= object | array | string | number | value_7 ws \n",
      "object ::= [{] ws object_16 [}] ws \n",
      "array ::= [[] ws array_20 []] ws \n",
      "string ::= [\"] string_23 [\"] ws \n",
      "number ::= number_24 number_30 number_34 ws \n",
      "value_7 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_36 \n",
      "arr_9 ::= value arr_11 \n",
      "arr_10 ::= [,] [<U+000A>] ws value \n",
      "arr_11 ::= arr_10 arr_11 | \n",
      "arr_12 ::= arr_9 | \n",
      "object_13 ::= string [:] ws value object_15 \n",
      "object_14 ::= [,] ws string [:] ws value \n",
      "object_15 ::= object_14 object_15 | \n",
      "object_16 ::= object_13 | \n",
      "array_17 ::= value array_19 \n",
      "array_18 ::= [,] ws value \n",
      "array_19 ::= array_18 array_19 | \n",
      "array_20 ::= array_17 | \n",
      "string_21 ::= [^\"\\] | [\\] string_22 \n",
      "string_22 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_23 ::= string_21 string_23 | \n",
      "number_24 ::= number_25 number_26 \n",
      "number_25 ::= [-] | \n",
      "number_26 ::= [0-9] | [1-9] number_27 \n",
      "number_27 ::= [0-9] number_27 | \n",
      "number_28 ::= [.] number_29 \n",
      "number_29 ::= [0-9] number_29 | [0-9] \n",
      "number_30 ::= number_28 | \n",
      "number_31 ::= [eE] number_32 number_33 \n",
      "number_32 ::= [-+] | \n",
      "number_33 ::= [0-9] number_33 | [0-9] \n",
      "number_34 ::= number_31 | \n",
      "ws_35 ::= [ <U+0009><U+000A>] ws \n",
      "ws_36 ::= ws_35 | \n",
      "\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from openhermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = teknium_openhermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1012.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   545.82 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n \"1706.03762\",\\n \"1804.02391\",\\n \"1810.13409\",\\n \"1810.10126\"\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6855.33 ms\n",
      "llama_print_timings:      sample time =     271.12 ms /    55 runs   (    4.93 ms per token,   202.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38534.66 ms /  2402 tokens (   16.04 ms per token,    62.33 tokens per second)\n",
      "llama_print_timings:        eval time =    2956.72 ms /    54 runs   (   54.75 ms per token,    18.26 tokens per second)\n",
      "/var/folders/jq/w3ddfwz910xdn6p611vx4sjh0000gn/T/ipykernel_91434/3247289090.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['id'] = pd.Categorical(filtered_df['id'], categories=result_ids, ordered=True)\n",
      "llama_print_timings:       total time =   41870.97 ms /  2456 tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>versions</th>\n",
       "      <th>source</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1706.03762</td>\n",
       "      <td>Ashish Vaswani</td>\n",
       "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\n  Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>15 pages, 5 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL cs.LG]</td>\n",
       "      <td>[v1, v2, v3, v4, v5]</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1706.03762\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1804.02391</td>\n",
       "      <td>Saumya Jetley</td>\n",
       "      <td>Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H.S. Torr</td>\n",
       "      <td>Learn To Pay Attention</td>\n",
       "      <td>International Conference on Learning Representations 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  We propose an end-to-end-trainable attention module for convolutional neural\\nnetwork (CNN) architectures built for image classification. The module takes as\\ninput the 2D feature vector maps which form the intermediate representations of\\nthe input image at different stages in the CNN pipeline, and outputs a 2D\\nmatrix of scores for each map. Standard CNN architectures are modified through\\nthe incorporation of this module, and trained under the constraint that a\\nconvex combination of the intermediate 2D feature vectors, as parameterised by\\nthe score matrices, must \\textit{alone} be used for classification.\\nIncentivised to amplify the relevant and suppress the irrelevant or misleading,\\nthe scores thus assume the role of attention values. Our experimental\\nobservations provide clear evidence to this effect: the learned attention maps\\nneatly highlight the regions of interest while suppressing background clutter.\\nConsequently, the proposed function is able to bootstrap standard CNN\\narchitectures for the task of image classification, demonstrating superior\\ngeneralisation over 6 unseen benchmark datasets. When binarised, our attention\\nmaps outperform other CNN-based attention maps, traditional saliency maps, and\\ntop object proposals for weakly supervised segmentation as demonstrated on the\\nObject Discovery dataset. We also demonstrate improved robustness against the\\nfast gradient sign method of adversarial attack.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CV cs.AI]</td>\n",
       "      <td>[v1, v2]</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1804.02391\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1810.13409</td>\n",
       "      <td>Ofir Press</td>\n",
       "      <td>Ofir Press, Noah A. Smith</td>\n",
       "      <td>You May Not Need Attention</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  In NMT, how far can we get without attention and without separate encoding\\nand decoding? To answer that question, we introduce a recurrent neural\\ntranslation model that does not use attention and does not have a separate\\nencoder and decoder. Our eager translation model is low-latency, writing target\\ntokens as soon as it reads the first source token, and uses constant memory\\nduring decoding. It performs on par with the standard attention-based model of\\nBahdanau et al. (2014), and better on long sentences.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1810.13409\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1810.10126</td>\n",
       "      <td>Yang Li</td>\n",
       "      <td>Yang Li, Lukasz Kaiser, Samy Bengio, Si Si</td>\n",
       "      <td>Area Attention</td>\n",
       "      <td>8 pages plus references</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  Existing attention mechanisms are trained to attend to individual items in a\\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\\ntoken or an image grid. We propose area attention: a way to attend to areas in\\nthe memory, where each area contains a group of items that are structurally\\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\\n1D memory such as natural language sentences. Importantly, the shape and the\\nsize of an area are dynamically determined via learning, which enables a model\\nto attend to information with varying granularity. Area attention can easily\\nwork with existing model architectures such as multi-head attention for\\nsimultaneously attending to multiple areas in the memory. We evaluate area\\nattention on two tasks: neural machine translation (both character and\\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\\nbaselines in all the cases. These improvements are obtainable with a basic form\\nof area attention that is parameter free.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.LG cs.AI cs.CL stat.ML]</td>\n",
       "      <td>[v1, v2, v3, v4, v5, v6]</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1810.10126\">Link</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp import LlamaGrammar\n",
    "import pandas as pd\n",
    "import json\n",
    "import httpx\n",
    "grammar_text = httpx.get(\"https://raw.githubusercontent.com/ggerganov/llama.cpp/master/grammars/json_arr.gbnf\").text\n",
    "grammar = LlamaGrammar.from_string(grammar_text)\n",
    "\n",
    "model = \"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\n",
    "prompt = f\"\"\"You are an expert at generating valid JSON.\n",
    "###\n",
    "Instruction:\n",
    "Return a valid JSON Array containing arXiv ['id'] field reranked according to how relevant the result is to the query based on its other columns at that ['id']. Drop any items that are not relevant to the query. Return just an array of the IDs, like [x,y,z] and so on in the correct order:\n",
    "        INDEX: {df[['id', 'title', 'abstract']].to_html(escape=False)}\n",
    "        QUERY: {query}\n",
    "        Take a deep breath, and solve the problem step-by-step.\n",
    "###\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "llm = Llama(model_path=model, n_ctx=8096, last_n_tokens_size=256, n_threads=4, n_gpu_layers=0)\n",
    "\n",
    "    \n",
    "stream = llm.create_completion(prompt, stream=True, repeat_penalty=1.1, max_tokens=256, stop=[\"]\"], echo=False, temperature=0, mirostat_mode = 2, mirostat_tau=4.0, mirostat_eta=1.1, grammar=grammar)\n",
    "result = \"\"\n",
    "for output in stream:\n",
    "    result += output['choices'][0]['text']\n",
    "\n",
    "result = result + \"]\"\n",
    "\n",
    "# Check if the result is a string, an array string, or a single ID in an array and convert it to a list of IDs\n",
    "if isinstance(result, str):\n",
    "    result_ids = [result.strip('[]')]\n",
    "elif isinstance(result, list):\n",
    "    if isinstance(result[0], str):\n",
    "        result_ids = [json.loads(res) for res in result]\n",
    "    else:\n",
    "        result_ids = result\n",
    "# Print the result\n",
    "print(result_ids)\n",
    "import re\n",
    "\n",
    "# Extract IDs from the potentially broken string using regex\n",
    "result_ids = re.findall(r'\"(.*?)\"', result_ids[0])\n",
    "\n",
    "# Filter the dataframe to only include rows with IDs in the result\n",
    "filtered_df = df[df['id'].isin(result_ids)]\n",
    "\n",
    "# Create a categorical type for sorting based on the order in result_ids\n",
    "filtered_df['id'] = pd.Categorical(filtered_df['id'], categories=result_ids, ordered=True)\n",
    "\n",
    "# Sort the dataframe based on the 'id' column\n",
    "filtered_df = filtered_df.sort_values('id')\n",
    "\n",
    "# Drop the similarity score column\n",
    "filtered_df = filtered_df.drop(columns=['similarity_score'])\n",
    "\n",
    "# Display the filtered dataframe as a table with hyperlinks\n",
    "display(HTML(filtered_df.to_html(escape=False)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
