{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suggest using Python 3.10 in a conda environment with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab a random Dataclysm arXiv paper's PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.25.0 (from -r requirements.txt (line 1))\n",
      "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting aiofiles==23.2.1 (from -r requirements.txt (line 2))\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: aiohttp==3.9.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.9.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.3.1)\n",
      "Collecting annotated-types==0.6.0 (from -r requirements.txt (line 5))\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting anyio==4.2.0 (from -r requirements.txt (line 6))\n",
      "  Using cached anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting apache-beam==2.52.0 (from -r requirements.txt (line 7))\n",
      "  Using cached apache-beam-2.52.0.tar.gz (2.4 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting appdirs==1.4.4 (from -r requirements.txt (line 8))\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: appnope==0.1.3 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.1.3)\n",
      "Collecting asgiref==3.7.2 (from -r requirements.txt (line 11))\n",
      "  Using cached asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting astor==0.8.1 (from -r requirements.txt (line 12))\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: asttokens==2.4.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.4.1)\n",
      "Requirement already satisfied: attrs==23.2.0 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (23.2.0)\n",
      "Collecting backoff==2.2.1 (from -r requirements.txt (line 15))\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting beautifulsoup4==4.12.2 (from -r requirements.txt (line 16))\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting bitsandbytes==0.42.0 (from -r requirements.txt (line 17))\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting blessed==1.20.0 (from -r requirements.txt (line 18))\n",
      "  Using cached blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting boto==2.49.0 (from -r requirements.txt (line 19))\n",
      "  Using cached boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting build==1.0.3 (from -r requirements.txt (line 20))\n",
      "  Using cached build-1.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting CacheControl==0.13.1 (from -r requirements.txt (line 21))\n",
      "  Using cached cachecontrol-0.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cachetools==5.3.2 (from -r requirements.txt (line 22))\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: certifi==2023.11.17 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (3.3.2)\n",
      "Collecting ci-info==0.3.0 (from -r requirements.txt (line 26))\n",
      "  Using cached ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting cleo==2.1.0 (from -r requirements.txt (line 27))\n",
      "  Using cached cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: click==8.1.7 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (8.1.7)\n",
      "Collecting cloudpickle==2.2.1 (from -r requirements.txt (line 29))\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting colorama==0.4.6 (from -r requirements.txt (line 30))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting comm==0.2.0 (from -r requirements.txt (line 31))\n",
      "  Using cached comm-0.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting configobj==5.0.8 (from -r requirements.txt (line 32))\n",
      "  Using cached configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
      "Collecting configparser==6.0.0 (from -r requirements.txt (line 33))\n",
      "  Using cached configparser-6.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting contourpy==1.2.0 (from -r requirements.txt (line 34))\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting crashtest==0.4.1 (from -r requirements.txt (line 35))\n",
      "  Using cached crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting crcmod==1.7 (from -r requirements.txt (line 36))\n",
      "  Using cached crcmod-1.7.tar.gz (89 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cryptography==41.0.7 (from -r requirements.txt (line 37))\n",
      "  Using cached cryptography-41.0.7-cp37-abi3-macosx_10_12_universal2.whl.metadata (5.2 kB)\n",
      "Collecting cycler==0.12.1 (from -r requirements.txt (line 38))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dataclasses==0.6 (from -r requirements.txt (line 39))\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting dataclasses-json==0.6.3 (from -r requirements.txt (line 40))\n",
      "  Using cached dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting datasets==2.16.1 (from -r requirements.txt (line 41))\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting debugpy==1.8.0 (from -r requirements.txt (line 42))\n",
      "  Downloading debugpy-1.8.0-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (5.1.1)\n",
      "Collecting Deprecated==1.2.14 (from -r requirements.txt (line 44))\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: dill==0.3.7 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (0.3.7)\n",
      "Collecting diskcache==5.6.3 (from -r requirements.txt (line 46))\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting distlib==0.3.8 (from -r requirements.txt (line 47))\n",
      "  Using cached distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting distro==1.9.0 (from -r requirements.txt (line 48))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dnspython==2.4.2 (from -r requirements.txt (line 49))\n",
      "  Using cached dnspython-2.4.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting docarray==0.40.0 (from -r requirements.txt (line 50))\n",
      "  Using cached docarray-0.40.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting docker==7.0.0 (from -r requirements.txt (line 51))\n",
      "  Using cached docker-7.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting docker-pycreds==0.4.0 (from -r requirements.txt (line 52))\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting docopt==0.6.2 (from -r requirements.txt (line 53))\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dulwich==0.21.7 (from -r requirements.txt (line 55))\n",
      "  Downloading dulwich-0.21.7-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
      "Collecting ecdsa==0.18.0 (from -r requirements.txt (line 56))\n",
      "  Using cached ecdsa-0.18.0-py2.py3-none-any.whl (142 kB)\n",
      "Collecting editor==1.6.5 (from -r requirements.txt (line 57))\n",
      "  Using cached editor-1.6.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting etelemetry==0.3.1 (from -r requirements.txt (line 58))\n",
      "  Using cached etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: executing==2.0.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 59)) (2.0.1)\n",
      "Collecting fastapi==0.108.0 (from -r requirements.txt (line 60))\n",
      "  Using cached fastapi-0.108.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting fastavro==1.9.2 (from -r requirements.txt (line 61))\n",
      "  Downloading fastavro-1.9.2.tar.gz (976 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m976.9/976.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fasteners==0.19 (from -r requirements.txt (line 62))\n",
      "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting fastjsonschema==2.19.1 (from -r requirements.txt (line 63))\n",
      "  Using cached fastjsonschema-2.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: filelock==3.13.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 64)) (3.13.1)\n",
      "Collecting fitz==0.0.1.dev2 (from -r requirements.txt (line 65))\n",
      "  Using cached fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Collecting fonttools==4.47.0 (from -r requirements.txt (line 66))\n",
      "  Downloading fonttools-4.47.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frontend==0.0.3 (from -r requirements.txt (line 67))\n",
      "  Using cached frontend-0.0.3-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: frozenlist==1.4.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 68)) (1.4.1)\n",
      "Requirement already satisfied: fsspec==2023.10.0 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 69)) (2023.10.0)\n",
      "Collecting future==0.18.3 (from -r requirements.txt (line 70))\n",
      "  Using cached future-0.18.3.tar.gz (840 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gcs-oauth2-boto-plugin==3.0 (from -r requirements.txt (line 71))\n",
      "  Using cached gcs-oauth2-boto-plugin-3.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git-python==1.0.3 (from -r requirements.txt (line 72))\n",
      "  Using cached git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting gitdb==4.0.11 (from -r requirements.txt (line 73))\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting GitPython==3.1.40 (from -r requirements.txt (line 74))\n",
      "  Using cached GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting google-apitools==0.5.32 (from -r requirements.txt (line 75))\n",
      "  Using cached google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
      "Collecting google-auth==2.26.2 (from -r requirements.txt (line 76))\n",
      "  Using cached google_auth-2.26.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-reauth==0.1.1 (from -r requirements.txt (line 77))\n",
      "  Using cached google_reauth-0.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting googleapis-common-protos==1.62.0 (from -r requirements.txt (line 78))\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting greenlet==3.0.3 (from -r requirements.txt (line 79))\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting grpcio==1.57.0 (from -r requirements.txt (line 80))\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-health-checking==1.57.0 (from -r requirements.txt (line 81))\n",
      "  Using cached grpcio_health_checking-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting grpcio-reflection==1.57.0 (from -r requirements.txt (line 82))\n",
      "  Using cached grpcio_reflection-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting gsutil==5.27 (from -r requirements.txt (line 83))\n",
      "  Using cached gsutil-5.27.tar.gz (3.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting h11==0.14.0 (from -r requirements.txt (line 84))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting hdfs==2.7.3 (from -r requirements.txt (line 85))\n",
      "  Using cached hdfs-2.7.3.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hf_transfer==0.1.4 (from -r requirements.txt (line 86))\n",
      "  Downloading hf_transfer-0.1.4-cp310-cp310-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata (1.5 kB)\n",
      "Collecting html2image==2.0.4.3 (from -r requirements.txt (line 87))\n",
      "  Using cached html2image-2.0.4.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting httpcore==1.0.2 (from -r requirements.txt (line 88))\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting httplib2==0.20.4 (from -r requirements.txt (line 89))\n",
      "  Using cached httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "Collecting httptools==0.6.1 (from -r requirements.txt (line 90))\n",
      "  Downloading httptools-0.6.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Collecting httpx==0.26.0 (from -r requirements.txt (line 91))\n",
      "  Using cached httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub==0.20.1 (from -r requirements.txt (line 92))\n",
      "  Using cached huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: idna==3.6 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 93)) (3.6)\n",
      "Collecting importlib-metadata==6.11.0 (from -r requirements.txt (line 94))\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting inquirer==3.2.1 (from -r requirements.txt (line 95))\n",
      "  Using cached inquirer-3.2.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting installer==0.7.0 (from -r requirements.txt (line 96))\n",
      "  Using cached installer-0.7.0-py3-none-any.whl (453 kB)\n",
      "Collecting ipykernel==6.28.0 (from -r requirements.txt (line 97))\n",
      "  Using cached ipykernel-6.28.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting ipython==8.19.0 (from -r requirements.txt (line 98))\n",
      "  Using cached ipython-8.19.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting isodate==0.6.1 (from -r requirements.txt (line 99))\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting itsdangerous==2.1.2 (from -r requirements.txt (line 100))\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting jaraco.classes==3.3.0 (from -r requirements.txt (line 101))\n",
      "  Using cached jaraco.classes-3.3.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jcloud==0.3 (from -r requirements.txt (line 102))\n",
      "  Using cached jcloud-0.3.tar.gz (39 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jedi==0.19.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 103)) (0.19.1)\n",
      "Collecting jina==3.23.2 (from -r requirements.txt (line 104))\n",
      "  Downloading jina-3.23.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (38 kB)\n",
      "Collecting jina-hubble-sdk==0.39.0 (from -r requirements.txt (line 105))\n",
      "  Using cached jina_hubble_sdk-0.39.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting Jinja2==3.1.2 (from -r requirements.txt (line 106))\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: joblib==1.3.2 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 107)) (1.3.2)\n",
      "Collecting Js2Py==0.74 (from -r requirements.txt (line 108))\n",
      "  Using cached Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
      "Collecting jsonschema==4.20.0 (from -r requirements.txt (line 109))\n",
      "  Using cached jsonschema-4.20.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting jsonschema-specifications==2023.12.1 (from -r requirements.txt (line 110))\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jupyter_client==8.6.0 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 111)) (8.6.0)\n",
      "Collecting jupyter_core==5.5.1 (from -r requirements.txt (line 112))\n",
      "  Using cached jupyter_core-5.5.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting keyring==24.3.0 (from -r requirements.txt (line 113))\n",
      "  Using cached keyring-24.3.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting kiwisolver==1.4.5 (from -r requirements.txt (line 114))\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting litellm==1.16.19 (from -r requirements.txt (line 115))\n",
      "  Using cached litellm-1.16.19-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llama-index==0.9.24 (from -r requirements.txt (line 116))\n",
      "  Using cached llama_index-0.9.24-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting llama_cpp_python==0.2.26 (from -r requirements.txt (line 117))\n",
      "  Using cached llama_cpp_python-0.2.26.tar.gz (8.8 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting looseversion==1.3.0 (from -r requirements.txt (line 118))\n",
      "  Using cached looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting lxml==5.0.0 (from -r requirements.txt (line 119))\n",
      "  Downloading lxml-5.0.0.zip (4.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting markdown-it-py==3.0.0 (from -r requirements.txt (line 120))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: MarkupSafe==2.1.3 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 121)) (2.1.3)\n",
      "Collecting marshmallow==3.20.1 (from -r requirements.txt (line 122))\n",
      "  Using cached marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting matplotlib==3.8.2 (from -r requirements.txt (line 123))\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 124)) (0.1.6)\n",
      "Collecting mdurl==0.1.2 (from -r requirements.txt (line 125))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting mlx==0.0.6 (from -r requirements.txt (line 126))\n",
      "  Downloading mlx-0.0.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting monotonic==1.6 (from -r requirements.txt (line 127))\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting more-itertools==10.1.0 (from -r requirements.txt (line 128))\n",
      "  Using cached more_itertools-10.1.0-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting MouseInfo==0.1.3 (from -r requirements.txt (line 129))\n",
      "  Using cached MouseInfo-0.1.3.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: mpmath==1.3.0 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 130)) (1.3.0)\n",
      "Collecting msgpack==1.0.7 (from -r requirements.txt (line 131))\n",
      "  Downloading msgpack-1.0.7-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: multidict==6.0.4 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 132)) (6.0.4)\n",
      "Requirement already satisfied: multiprocess==0.70.15 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 133)) (0.70.15)\n",
      "Collecting mwparserfromhell==0.6.5 (from -r requirements.txt (line 134))\n",
      "  Downloading mwparserfromhell-0.6.5.tar.gz (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mypy-extensions==1.0.0 (from -r requirements.txt (line 135))\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting nest-asyncio==1.5.8 (from -r requirements.txt (line 136))\n",
      "  Using cached nest_asyncio-1.5.8-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: networkx==3.2.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 137)) (3.2.1)\n",
      "Collecting nibabel==5.2.0 (from -r requirements.txt (line 138))\n",
      "  Using cached nibabel-5.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting nipype==1.8.6 (from -r requirements.txt (line 139))\n",
      "  Using cached nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
      "Requirement already satisfied: nltk==3.8.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 140)) (3.8.1)\n",
      "Collecting numpy==1.26.2 (from -r requirements.txt (line 141))\n",
      "  Downloading numpy-1.26.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauth2client==4.1.3 (from -r requirements.txt (line 142))\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting objsize==0.6.1 (from -r requirements.txt (line 143))\n",
      "  Using cached objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting open-interpreter==0.2.0 (from -r requirements.txt (line 144))\n",
      "  Using cached open_interpreter-0.2.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting openai==1.6.1 (from -r requirements.txt (line 145))\n",
      "  Using cached openai-1.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting opencv-python==4.9.0.80 (from -r requirements.txt (line 146))\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting opentelemetry-api==1.19.0 (from -r requirements.txt (line 147))\n",
      "  Using cached opentelemetry_api-1.19.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp==1.19.0 (from -r requirements.txt (line 148))\n",
      "  Using cached opentelemetry_exporter_otlp-1.19.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.19.0 (from -r requirements.txt (line 149))\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.19.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.19.0 (from -r requirements.txt (line 150))\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.19.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.19.0 (from -r requirements.txt (line 151))\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.19.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-prometheus==0.41b0 (from -r requirements.txt (line 152))\n",
      "  Using cached opentelemetry_exporter_prometheus-0.41b0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-instrumentation==0.40b0 (from -r requirements.txt (line 153))\n",
      "  Using cached opentelemetry_instrumentation-0.40b0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting opentelemetry-instrumentation-aiohttp-client==0.40b0 (from -r requirements.txt (line 154))\n",
      "  Using cached opentelemetry_instrumentation_aiohttp_client-0.40b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.40b0 (from -r requirements.txt (line 155))\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.40b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi==0.40b0 (from -r requirements.txt (line 156))\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.40b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-grpc==0.40b0 (from -r requirements.txt (line 157))\n",
      "  Using cached opentelemetry_instrumentation_grpc-0.40b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-proto==1.19.0 (from -r requirements.txt (line 158))\n",
      "  Using cached opentelemetry_proto-1.19.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk==1.19.0 (from -r requirements.txt (line 159))\n",
      "  Using cached opentelemetry_sdk-1.19.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.40b0 (from -r requirements.txt (line 160))\n",
      "  Using cached opentelemetry_semantic_conventions-0.40b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.40b0 (from -r requirements.txt (line 161))\n",
      "  Using cached opentelemetry_util_http-0.40b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting orjson==3.9.10 (from -r requirements.txt (line 162))\n",
      "  Downloading orjson-3.9.10-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging==23.2 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 163)) (23.2)\n",
      "Requirement already satisfied: pandas==2.1.4 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 164)) (2.1.4)\n",
      "Requirement already satisfied: parso==0.8.3 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 165)) (0.8.3)\n",
      "Collecting pathlib==1.0.1 (from -r requirements.txt (line 166))\n",
      "  Using cached pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Collecting pathspec==0.12.1 (from -r requirements.txt (line 167))\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pdfminer.six==20221105 (from -r requirements.txt (line 168))\n",
      "  Using cached pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "Collecting pdfplumber==0.10.3 (from -r requirements.txt (line 169))\n",
      "  Using cached pdfplumber-0.10.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting peft==0.7.1 (from -r requirements.txt (line 170))\n",
      "  Using cached peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pexpect==4.9.0 (from -r requirements.txt (line 171))\n",
      "  Using cached pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Pillow==10.1.0 (from -r requirements.txt (line 172))\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Collecting pkginfo==1.9.6 (from -r requirements.txt (line 173))\n",
      "  Using cached pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
      "Collecting platformdirs==4.0.0 (from -r requirements.txt (line 174))\n",
      "  Using cached platformdirs-4.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting plyer==2.1.0 (from -r requirements.txt (line 175))\n",
      "  Using cached plyer-2.1.0-py2.py3-none-any.whl (142 kB)\n",
      "Collecting poetry==1.7.1 (from -r requirements.txt (line 176))\n",
      "  Using cached poetry-1.7.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting poetry-core==1.8.1 (from -r requirements.txt (line 177))\n",
      "  Using cached poetry_core-1.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting poetry-plugin-export==1.6.0 (from -r requirements.txt (line 178))\n",
      "  Using cached poetry_plugin_export-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting posthog==3.1.0 (from -r requirements.txt (line 179))\n",
      "  Using cached posthog-3.1.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pretty-traceback==2023.1020 (from -r requirements.txt (line 180))\n",
      "  Using cached pretty_traceback-2023.1020-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting prometheus-client==0.19.0 (from -r requirements.txt (line 181))\n",
      "  Using cached prometheus_client-0.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting prompt-toolkit==3.0.43 (from -r requirements.txt (line 182))\n",
      "  Using cached prompt_toolkit-3.0.43-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting proto-plus==1.23.0 (from -r requirements.txt (line 183))\n",
      "  Using cached proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf==4.25.1 (from -r requirements.txt (line 184))\n",
      "  Using cached protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting prov==2.0.0 (from -r requirements.txt (line 185))\n",
      "  Using cached prov-2.0.0-py3-none-any.whl (421 kB)\n",
      "Collecting psutil==5.9.7 (from -r requirements.txt (line 186))\n",
      "  Using cached psutil-5.9.7-cp38-abi3-macosx_11_0_arm64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 187)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 188)) (0.2.2)\n",
      "Collecting pyarrow==11.0.0 (from -r requirements.txt (line 189))\n",
      "  Downloading pyarrow-11.0.0-cp310-cp310-macosx_11_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow-hotfix==0.6 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 190)) (0.6)\n",
      "Collecting pyasn1==0.5.1 (from -r requirements.txt (line 191))\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyasn1-modules==0.3.0 (from -r requirements.txt (line 192))\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting PyAutoGUI==0.9.54 (from -r requirements.txt (line 193))\n",
      "  Using cached PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydantic==2.5.3 (from -r requirements.txt (line 194))\n",
      "  Using cached pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pydantic-settings==2.1.0 (from -r requirements.txt (line 195))\n",
      "  Using cached pydantic_settings-2.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pydantic_core==2.14.6 (from -r requirements.txt (line 196))\n",
      "  Downloading pydantic_core-2.14.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting pydot==1.4.2 (from -r requirements.txt (line 197))\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting PyGetWindow==0.0.9 (from -r requirements.txt (line 198))\n",
      "  Using cached PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pygments==2.17.2 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 199)) (2.17.2)\n",
      "Collecting pyjsparser==2.7.1 (from -r requirements.txt (line 200))\n",
      "  Using cached pyjsparser-2.7.1.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting PyMonCtl==0.7 (from -r requirements.txt (line 201))\n",
      "  Using cached PyMonCtl-0.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pymongo==4.6.1 (from -r requirements.txt (line 202))\n",
      "  Downloading pymongo-4.6.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (22 kB)\n",
      "Collecting PyMsgBox==1.0.9 (from -r requirements.txt (line 203))\n",
      "  Using cached PyMsgBox-1.0.9.tar.gz (18 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyobjc==10.1 (from -r requirements.txt (line 204))\n",
      "  Using cached pyobjc-10.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pyobjc-core==10.1 (from -r requirements.txt (line 205))\n",
      "  Downloading pyobjc_core-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-Accessibility==10.1 (from -r requirements.txt (line 206))\n",
      "  Using cached pyobjc_framework_Accessibility-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Accounts==10.1 (from -r requirements.txt (line 207))\n",
      "  Using cached pyobjc_framework_Accounts-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-AddressBook==10.1 (from -r requirements.txt (line 208))\n",
      "  Using cached pyobjc_framework_AddressBook-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-AdServices==10.1 (from -r requirements.txt (line 209))\n",
      "  Using cached pyobjc_framework_AdServices-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-AdSupport==10.1 (from -r requirements.txt (line 210))\n",
      "  Using cached pyobjc_framework_AdSupport-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-AppleScriptKit==10.1 (from -r requirements.txt (line 211))\n",
      "  Using cached pyobjc_framework_AppleScriptKit-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-AppleScriptObjC==10.1 (from -r requirements.txt (line 212))\n",
      "  Using cached pyobjc_framework_AppleScriptObjC-10.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-ApplicationServices==10.1 (from -r requirements.txt (line 213))\n",
      "  Downloading pyobjc_framework_ApplicationServices-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-AppTrackingTransparency==10.1 (from -r requirements.txt (line 214))\n",
      "  Using cached pyobjc_framework_AppTrackingTransparency-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-AudioVideoBridging==10.1 (from -r requirements.txt (line 215))\n",
      "  Downloading pyobjc_framework_AudioVideoBridging-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-AuthenticationServices==10.1 (from -r requirements.txt (line 216))\n",
      "  Using cached pyobjc_framework_AuthenticationServices-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-AutomaticAssessmentConfiguration==10.1 (from -r requirements.txt (line 217))\n",
      "  Using cached pyobjc_framework_AutomaticAssessmentConfiguration-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Automator==10.1 (from -r requirements.txt (line 218))\n",
      "  Using cached pyobjc_framework_Automator-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-AVFoundation==10.1 (from -r requirements.txt (line 219))\n",
      "  Using cached pyobjc_framework_AVFoundation-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-AVKit==10.1 (from -r requirements.txt (line 220))\n",
      "  Using cached pyobjc_framework_AVKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-AVRouting==10.1 (from -r requirements.txt (line 221))\n",
      "  Using cached pyobjc_framework_AVRouting-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-BackgroundAssets==10.1 (from -r requirements.txt (line 222))\n",
      "  Using cached pyobjc_framework_BackgroundAssets-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-BusinessChat==10.1 (from -r requirements.txt (line 223))\n",
      "  Using cached pyobjc_framework_BusinessChat-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CalendarStore==10.1 (from -r requirements.txt (line 224))\n",
      "  Using cached pyobjc_framework_CalendarStore-10.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-CallKit==10.1 (from -r requirements.txt (line 225))\n",
      "  Using cached pyobjc_framework_CallKit-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CFNetwork==10.1 (from -r requirements.txt (line 226))\n",
      "  Using cached pyobjc_framework_CFNetwork-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-Cinematic==10.1 (from -r requirements.txt (line 227))\n",
      "  Using cached pyobjc_framework_Cinematic-10.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-ClassKit==10.1 (from -r requirements.txt (line 228))\n",
      "  Using cached pyobjc_framework_ClassKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CloudKit==10.1 (from -r requirements.txt (line 229))\n",
      "  Using cached pyobjc_framework_CloudKit-10.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-Cocoa==10.1 (from -r requirements.txt (line 230))\n",
      "  Downloading pyobjc_framework_Cocoa-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Collaboration==10.1 (from -r requirements.txt (line 231))\n",
      "  Using cached pyobjc_framework_Collaboration-10.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-ColorSync==10.1 (from -r requirements.txt (line 232))\n",
      "  Using cached pyobjc_framework_ColorSync-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Contacts==10.1 (from -r requirements.txt (line 233))\n",
      "  Using cached pyobjc_framework_Contacts-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-ContactsUI==10.1 (from -r requirements.txt (line 234))\n",
      "  Using cached pyobjc_framework_ContactsUI-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-CoreAudio==10.1 (from -r requirements.txt (line 235))\n",
      "  Downloading pyobjc_framework_CoreAudio-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CoreAudioKit==10.1 (from -r requirements.txt (line 236))\n",
      "  Using cached pyobjc_framework_CoreAudioKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-CoreBluetooth==10.1 (from -r requirements.txt (line 237))\n",
      "  Using cached pyobjc_framework_CoreBluetooth-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-CoreData==10.1 (from -r requirements.txt (line 238))\n",
      "  Using cached pyobjc_framework_CoreData-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-CoreHaptics==10.1 (from -r requirements.txt (line 239))\n",
      "  Using cached pyobjc_framework_CoreHaptics-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CoreLocation==10.1 (from -r requirements.txt (line 240))\n",
      "  Using cached pyobjc_framework_CoreLocation-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-CoreMedia==10.1 (from -r requirements.txt (line 241))\n",
      "  Downloading pyobjc_framework_CoreMedia-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CoreMediaIO==10.1 (from -r requirements.txt (line 242))\n",
      "  Using cached pyobjc_framework_CoreMediaIO-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-CoreMIDI==10.1 (from -r requirements.txt (line 243))\n",
      "  Using cached pyobjc_framework_CoreMIDI-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CoreML==10.1 (from -r requirements.txt (line 244))\n",
      "  Using cached pyobjc_framework_CoreML-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CoreMotion==10.1 (from -r requirements.txt (line 245))\n",
      "  Downloading pyobjc_framework_CoreMotion-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CoreServices==10.1 (from -r requirements.txt (line 246))\n",
      "  Using cached pyobjc_framework_CoreServices-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-CoreSpotlight==10.1 (from -r requirements.txt (line 247))\n",
      "  Using cached pyobjc_framework_CoreSpotlight-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-CoreText==10.1 (from -r requirements.txt (line 248))\n",
      "  Downloading pyobjc_framework_CoreText-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-CoreWLAN==10.1 (from -r requirements.txt (line 249))\n",
      "  Using cached pyobjc_framework_CoreWLAN-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-CryptoTokenKit==10.1 (from -r requirements.txt (line 250))\n",
      "  Using cached pyobjc_framework_CryptoTokenKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-DataDetection==10.1 (from -r requirements.txt (line 251))\n",
      "  Using cached pyobjc_framework_DataDetection-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-DeviceCheck==10.1 (from -r requirements.txt (line 252))\n",
      "  Using cached pyobjc_framework_DeviceCheck-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-DictionaryServices==10.1 (from -r requirements.txt (line 253))\n",
      "  Using cached pyobjc_framework_DictionaryServices-10.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyobjc-framework-DiscRecording==10.1 (from -r requirements.txt (line 254))\n",
      "  Using cached pyobjc_framework_DiscRecording-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-DiscRecordingUI==10.1 (from -r requirements.txt (line 255))\n",
      "  Using cached pyobjc_framework_DiscRecordingUI-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-DiskArbitration==10.1 (from -r requirements.txt (line 256))\n",
      "  Using cached pyobjc_framework_DiskArbitration-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-DVDPlayback==10.1 (from -r requirements.txt (line 257))\n",
      "  Using cached pyobjc_framework_DVDPlayback-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-EventKit==10.1 (from -r requirements.txt (line 258))\n",
      "  Using cached pyobjc_framework_EventKit-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ExceptionHandling==10.1 (from -r requirements.txt (line 259))\n",
      "  Using cached pyobjc_framework_ExceptionHandling-10.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-ExecutionPolicy==10.1 (from -r requirements.txt (line 260))\n",
      "  Using cached pyobjc_framework_ExecutionPolicy-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ExtensionKit==10.1 (from -r requirements.txt (line 261))\n",
      "  Using cached pyobjc_framework_ExtensionKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ExternalAccessory==10.1 (from -r requirements.txt (line 262))\n",
      "  Using cached pyobjc_framework_ExternalAccessory-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-FileProvider==10.1 (from -r requirements.txt (line 263))\n",
      "  Downloading pyobjc_framework_FileProvider-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-FileProviderUI==10.1 (from -r requirements.txt (line 264))\n",
      "  Using cached pyobjc_framework_FileProviderUI-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-FinderSync==10.1 (from -r requirements.txt (line 265))\n",
      "  Using cached pyobjc_framework_FinderSync-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-FSEvents==10.1 (from -r requirements.txt (line 266))\n",
      "  Using cached pyobjc_framework_FSEvents-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-GameCenter==10.1 (from -r requirements.txt (line 267))\n",
      "  Using cached pyobjc_framework_GameCenter-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-GameController==10.1 (from -r requirements.txt (line 268))\n",
      "  Using cached pyobjc_framework_GameController-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-GameKit==10.1 (from -r requirements.txt (line 269))\n",
      "  Using cached pyobjc_framework_GameKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-GameplayKit==10.1 (from -r requirements.txt (line 270))\n",
      "  Using cached pyobjc_framework_GameplayKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-HealthKit==10.1 (from -r requirements.txt (line 271))\n",
      "  Using cached pyobjc_framework_HealthKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-ImageCaptureCore==10.1 (from -r requirements.txt (line 272))\n",
      "  Using cached pyobjc_framework_ImageCaptureCore-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-InputMethodKit==10.1 (from -r requirements.txt (line 273))\n",
      "  Using cached pyobjc_framework_InputMethodKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-InstallerPlugins==10.1 (from -r requirements.txt (line 274))\n",
      "  Using cached pyobjc_framework_InstallerPlugins-10.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-InstantMessage==10.1 (from -r requirements.txt (line 275))\n",
      "  Using cached pyobjc_framework_InstantMessage-10.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pyobjc-framework-Intents==10.1 (from -r requirements.txt (line 276))\n",
      "  Using cached pyobjc_framework_Intents-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-IntentsUI==10.1 (from -r requirements.txt (line 277))\n",
      "  Downloading pyobjc_framework_IntentsUI-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-IOBluetooth==10.1 (from -r requirements.txt (line 278))\n",
      "  Using cached pyobjc_framework_IOBluetooth-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-IOBluetoothUI==10.1 (from -r requirements.txt (line 279))\n",
      "  Using cached pyobjc_framework_IOBluetoothUI-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-IOSurface==10.1 (from -r requirements.txt (line 280))\n",
      "  Using cached pyobjc_framework_IOSurface-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-iTunesLibrary==10.1 (from -r requirements.txt (line 281))\n",
      "  Using cached pyobjc_framework_iTunesLibrary-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-KernelManagement==10.1 (from -r requirements.txt (line 282))\n",
      "  Using cached pyobjc_framework_KernelManagement-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-LatentSemanticMapping==10.1 (from -r requirements.txt (line 283))\n",
      "  Using cached pyobjc_framework_LatentSemanticMapping-10.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pyobjc-framework-LaunchServices==10.1 (from -r requirements.txt (line 284))\n",
      "  Using cached pyobjc_framework_LaunchServices-10.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyobjc-framework-libdispatch==10.1 (from -r requirements.txt (line 285))\n",
      "  Downloading pyobjc_framework_libdispatch-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-libxpc==10.1 (from -r requirements.txt (line 286))\n",
      "  Downloading pyobjc_framework_libxpc-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-LinkPresentation==10.1 (from -r requirements.txt (line 287))\n",
      "  Using cached pyobjc_framework_LinkPresentation-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-LocalAuthentication==10.1 (from -r requirements.txt (line 288))\n",
      "  Using cached pyobjc_framework_LocalAuthentication-10.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-LocalAuthenticationEmbeddedUI==10.1 (from -r requirements.txt (line 289))\n",
      "  Using cached pyobjc_framework_LocalAuthenticationEmbeddedUI-10.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-MailKit==10.1 (from -r requirements.txt (line 290))\n",
      "  Using cached pyobjc_framework_MailKit-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-MapKit==10.1 (from -r requirements.txt (line 291))\n",
      "  Using cached pyobjc_framework_MapKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-MediaAccessibility==10.1 (from -r requirements.txt (line 292))\n",
      "  Using cached pyobjc_framework_MediaAccessibility-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-MediaLibrary==10.1 (from -r requirements.txt (line 293))\n",
      "  Using cached pyobjc_framework_MediaLibrary-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-MediaPlayer==10.1 (from -r requirements.txt (line 294))\n",
      "  Using cached pyobjc_framework_MediaPlayer-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-MediaToolbox==10.1 (from -r requirements.txt (line 295))\n",
      "  Using cached pyobjc_framework_MediaToolbox-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Metal==10.1 (from -r requirements.txt (line 296))\n",
      "  Using cached pyobjc_framework_Metal-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-MetalFX==10.1 (from -r requirements.txt (line 297))\n",
      "  Using cached pyobjc_framework_MetalFX-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-MetalKit==10.1 (from -r requirements.txt (line 298))\n",
      "  Using cached pyobjc_framework_MetalKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-MetalPerformanceShaders==10.1 (from -r requirements.txt (line 299))\n",
      "  Using cached pyobjc_framework_MetalPerformanceShaders-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-MetalPerformanceShadersGraph==10.1 (from -r requirements.txt (line 300))\n",
      "  Using cached pyobjc_framework_MetalPerformanceShadersGraph-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-MetricKit==10.1 (from -r requirements.txt (line 301))\n",
      "  Downloading pyobjc_framework_MetricKit-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-MLCompute==10.1 (from -r requirements.txt (line 302))\n",
      "  Using cached pyobjc_framework_MLCompute-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-ModelIO==10.1 (from -r requirements.txt (line 303))\n",
      "  Using cached pyobjc_framework_ModelIO-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-MultipeerConnectivity==10.1 (from -r requirements.txt (line 304))\n",
      "  Using cached pyobjc_framework_MultipeerConnectivity-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-NaturalLanguage==10.1 (from -r requirements.txt (line 305))\n",
      "  Using cached pyobjc_framework_NaturalLanguage-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-NetFS==10.1 (from -r requirements.txt (line 306))\n",
      "  Using cached pyobjc_framework_NetFS-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-Network==10.1 (from -r requirements.txt (line 307))\n",
      "  Using cached pyobjc_framework_Network-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-NetworkExtension==10.1 (from -r requirements.txt (line 308))\n",
      "  Using cached pyobjc_framework_NetworkExtension-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-NotificationCenter==10.1 (from -r requirements.txt (line 309))\n",
      "  Using cached pyobjc_framework_NotificationCenter-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-OpenDirectory==10.1 (from -r requirements.txt (line 310))\n",
      "  Using cached pyobjc_framework_OpenDirectory-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-OSAKit==10.1 (from -r requirements.txt (line 311))\n",
      "  Using cached pyobjc_framework_OSAKit-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-OSLog==10.1 (from -r requirements.txt (line 312))\n",
      "  Using cached pyobjc_framework_OSLog-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-PassKit==10.1 (from -r requirements.txt (line 313))\n",
      "  Using cached pyobjc_framework_PassKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-PencilKit==10.1 (from -r requirements.txt (line 314))\n",
      "  Using cached pyobjc_framework_PencilKit-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-PHASE==10.1 (from -r requirements.txt (line 315))\n",
      "  Using cached pyobjc_framework_PHASE-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-Photos==10.1 (from -r requirements.txt (line 316))\n",
      "  Using cached pyobjc_framework_Photos-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-PhotosUI==10.1 (from -r requirements.txt (line 317))\n",
      "  Using cached pyobjc_framework_PhotosUI-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-PreferencePanes==10.1 (from -r requirements.txt (line 318))\n",
      "  Using cached pyobjc_framework_PreferencePanes-10.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-PushKit==10.1 (from -r requirements.txt (line 319))\n",
      "  Using cached pyobjc_framework_PushKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Quartz==10.1 (from -r requirements.txt (line 320))\n",
      "  Downloading pyobjc_framework_Quartz-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting pyobjc-framework-QuickLookThumbnailing==10.1 (from -r requirements.txt (line 321))\n",
      "  Using cached pyobjc_framework_QuickLookThumbnailing-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ReplayKit==10.1 (from -r requirements.txt (line 322))\n",
      "  Using cached pyobjc_framework_ReplayKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-SafariServices==10.1 (from -r requirements.txt (line 323))\n",
      "  Using cached pyobjc_framework_SafariServices-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-SafetyKit==10.1 (from -r requirements.txt (line 324))\n",
      "  Using cached pyobjc_framework_SafetyKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-SceneKit==10.1 (from -r requirements.txt (line 325))\n",
      "  Using cached pyobjc_framework_SceneKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ScreenCaptureKit==10.1 (from -r requirements.txt (line 326))\n",
      "  Downloading pyobjc_framework_ScreenCaptureKit-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ScreenSaver==10.1 (from -r requirements.txt (line 327))\n",
      "  Using cached pyobjc_framework_ScreenSaver-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ScreenTime==10.1 (from -r requirements.txt (line 328))\n",
      "  Using cached pyobjc_framework_ScreenTime-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-ScriptingBridge==10.1 (from -r requirements.txt (line 329))\n",
      "  Using cached pyobjc_framework_ScriptingBridge-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.6 kB)\n",
      "Collecting pyobjc-framework-SearchKit==10.1 (from -r requirements.txt (line 330))\n",
      "  Using cached pyobjc_framework_SearchKit-10.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyobjc-framework-Security==10.1 (from -r requirements.txt (line 331))\n",
      "  Downloading pyobjc_framework_Security-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-SecurityFoundation==10.1 (from -r requirements.txt (line 332))\n",
      "  Using cached pyobjc_framework_SecurityFoundation-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-SecurityInterface==10.1 (from -r requirements.txt (line 333))\n",
      "  Using cached pyobjc_framework_SecurityInterface-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-SensitiveContentAnalysis==10.1 (from -r requirements.txt (line 334))\n",
      "  Using cached pyobjc_framework_SensitiveContentAnalysis-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ServiceManagement==10.1 (from -r requirements.txt (line 335))\n",
      "  Using cached pyobjc_framework_ServiceManagement-10.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-SharedWithYou==10.1 (from -r requirements.txt (line 336))\n",
      "  Using cached pyobjc_framework_SharedWithYou-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-SharedWithYouCore==10.1 (from -r requirements.txt (line 337))\n",
      "  Using cached pyobjc_framework_SharedWithYouCore-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ShazamKit==10.1 (from -r requirements.txt (line 338))\n",
      "  Downloading pyobjc_framework_ShazamKit-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-Social==10.1 (from -r requirements.txt (line 339))\n",
      "  Using cached pyobjc_framework_Social-10.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyobjc-framework-SoundAnalysis==10.1 (from -r requirements.txt (line 340))\n",
      "  Using cached pyobjc_framework_SoundAnalysis-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Speech==10.1 (from -r requirements.txt (line 341))\n",
      "  Using cached pyobjc_framework_Speech-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-SpriteKit==10.1 (from -r requirements.txt (line 342))\n",
      "  Downloading pyobjc_framework_SpriteKit-10.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-StoreKit==10.1 (from -r requirements.txt (line 343))\n",
      "  Using cached pyobjc_framework_StoreKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Symbols==10.1 (from -r requirements.txt (line 344))\n",
      "  Using cached pyobjc_framework_Symbols-10.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyobjc-framework-SyncServices==10.1 (from -r requirements.txt (line 345))\n",
      "  Using cached pyobjc_framework_SyncServices-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.6 kB)\n",
      "Collecting pyobjc-framework-SystemConfiguration==10.1 (from -r requirements.txt (line 346))\n",
      "  Using cached pyobjc_framework_SystemConfiguration-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-SystemExtensions==10.1 (from -r requirements.txt (line 347))\n",
      "  Using cached pyobjc_framework_SystemExtensions-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-ThreadNetwork==10.1 (from -r requirements.txt (line 348))\n",
      "  Using cached pyobjc_framework_ThreadNetwork-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-UniformTypeIdentifiers==10.1 (from -r requirements.txt (line 349))\n",
      "  Using cached pyobjc_framework_UniformTypeIdentifiers-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-UserNotifications==10.1 (from -r requirements.txt (line 350))\n",
      "  Using cached pyobjc_framework_UserNotifications-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-UserNotificationsUI==10.1 (from -r requirements.txt (line 351))\n",
      "  Using cached pyobjc_framework_UserNotificationsUI-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-VideoSubscriberAccount==10.1 (from -r requirements.txt (line 352))\n",
      "  Using cached pyobjc_framework_VideoSubscriberAccount-10.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-VideoToolbox==10.1 (from -r requirements.txt (line 353))\n",
      "  Using cached pyobjc_framework_VideoToolbox-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyobjc-framework-Virtualization==10.1 (from -r requirements.txt (line 354))\n",
      "  Using cached pyobjc_framework_Virtualization-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-Vision==10.1 (from -r requirements.txt (line 355))\n",
      "  Using cached pyobjc_framework_Vision-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.3 kB)\n",
      "Collecting pyobjc-framework-WebKit==10.1 (from -r requirements.txt (line 356))\n",
      "  Using cached pyobjc_framework_WebKit-10.1-cp36-abi3-macosx_11_0_universal2.whl.metadata (2.4 kB)\n",
      "Collecting pyopencl==2023.1.4 (from -r requirements.txt (line 357))\n",
      "  Downloading pyopencl-2023.1.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.5 kB)\n",
      "Collecting pyOpenSSL==23.3.0 (from -r requirements.txt (line 358))\n",
      "  Using cached pyOpenSSL-23.3.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pypandoc==1.12 (from -r requirements.txt (line 359))\n",
      "  Using cached pypandoc-1.12-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pyparsing==3.1.1 (from -r requirements.txt (line 360))\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pypdf==3.17.4 (from -r requirements.txt (line 361))\n",
      "  Using cached pypdf-3.17.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting PyPDF2==3.0.1 (from -r requirements.txt (line 362))\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Collecting pypdfium2==4.25.0 (from -r requirements.txt (line 363))\n",
      "  Using cached pypdfium2-4.25.0-py3-none-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "Collecting pyperclip==1.8.2 (from -r requirements.txt (line 364))\n",
      "  Using cached pyperclip-1.8.2.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyproject_hooks==1.0.0 (from -r requirements.txt (line 365))\n",
      "  Using cached pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting PyRect==0.2.0 (from -r requirements.txt (line 366))\n",
      "  Using cached PyRect-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting PyScreeze==0.1.30 (from -r requirements.txt (line 367))\n",
      "  Using cached PyScreeze-0.1.30.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytesseract==0.3.10 (from -r requirements.txt (line 368))\n",
      "  Using cached pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 369)) (2.8.2)\n",
      "Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 370))\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting python-jose==3.3.0 (from -r requirements.txt (line 371))\n",
      "  Using cached python_jose-3.3.0-py2.py3-none-any.whl (33 kB)\n",
      "Collecting python-multipart==0.0.6 (from -r requirements.txt (line 372))\n",
      "  Using cached python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "Collecting pytils==0.4.1 (from -r requirements.txt (line 373))\n",
      "  Using cached pytils-0.4.1.tar.gz (99 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytools==2023.1.1 (from -r requirements.txt (line 374))\n",
      "  Using cached pytools-2023.1.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pytweening==1.0.7 (from -r requirements.txt (line 375))\n",
      "  Using cached pytweening-1.0.7.tar.gz (168 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytz==2023.3.post1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 376)) (2023.3.post1)\n",
      "Collecting pyu2f==0.1.5 (from -r requirements.txt (line 377))\n",
      "  Using cached pyu2f-0.1.5.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting PyWinBox==0.6 (from -r requirements.txt (line 378))\n",
      "  Using cached PyWinBox-0.6-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting PyWinCtl==0.3 (from -r requirements.txt (line 379))\n",
      "  Using cached PyWinCtl-0.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pyxnat==1.6 (from -r requirements.txt (line 380))\n",
      "  Using cached pyxnat-1.6-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 381)) (6.0.1)\n",
      "Collecting pyzmq==25.1.2 (from -r requirements.txt (line 382))\n",
      "  Downloading pyzmq-25.1.2-cp310-cp310-macosx_10_15_universal2.whl.metadata (4.9 kB)\n",
      "Collecting rapidfuzz==3.6.1 (from -r requirements.txt (line 383))\n",
      "  Downloading rapidfuzz-3.6.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting ray==2.9.0 (from -r requirements.txt (line 384))\n",
      "  Downloading ray-2.9.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting rdflib==7.0.0 (from -r requirements.txt (line 385))\n",
      "  Using cached rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting readchar==4.0.5 (from -r requirements.txt (line 386))\n",
      "  Using cached readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
      "Collecting referencing==0.32.0 (from -r requirements.txt (line 387))\n",
      "  Using cached referencing-0.32.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: regex==2023.12.25 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 388)) (2023.12.25)\n",
      "Requirement already satisfied: requests==2.31.0 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 389)) (2.31.0)\n",
      "Collecting requests-toolbelt==1.0.0 (from -r requirements.txt (line 390))\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Collecting retry-decorator==1.1.1 (from -r requirements.txt (line 391))\n",
      "  Using cached retry_decorator-1.1.1.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rich==13.7.0 (from -r requirements.txt (line 392))\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting rpds-py==0.16.2 (from -r requirements.txt (line 393))\n",
      "  Downloading rpds_py-0.16.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting rsa==4.7.2 (from -r requirements.txt (line 394))\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting rubicon-objc==0.4.7 (from -r requirements.txt (line 395))\n",
      "  Using cached rubicon_objc-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting runs==1.2.0 (from -r requirements.txt (line 396))\n",
      "  Using cached runs-1.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: safetensors==0.4.1 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 397)) (0.4.1)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 398)) (1.3.2)\n",
      "Requirement already satisfied: scipy==1.11.4 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 399)) (1.11.4)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 400)) (2.2.2)\n",
      "Requirement already satisfied: sentencepiece==0.1.99 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 401)) (0.1.99)\n",
      "Collecting sentry-sdk==1.39.2 (from -r requirements.txt (line 402))\n",
      "  Using cached sentry_sdk-1.39.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting setproctitle==1.3.3 (from -r requirements.txt (line 403))\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-macosx_10_9_universal2.whl.metadata (9.9 kB)\n",
      "Collecting shellingham==1.5.4 (from -r requirements.txt (line 404))\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting simplejson==3.19.2 (from -r requirements.txt (line 405))\n",
      "  Downloading simplejson-3.19.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: six==1.16.0 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 406)) (1.16.0)\n",
      "Collecting smmap==5.0.1 (from -r requirements.txt (line 407))\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting sniffio==1.3.0 (from -r requirements.txt (line 408))\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting soupsieve==2.5 (from -r requirements.txt (line 409))\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting SQLAlchemy==2.0.24 (from -r requirements.txt (line 410))\n",
      "  Downloading SQLAlchemy-2.0.24-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting sse-starlette==1.8.2 (from -r requirements.txt (line 411))\n",
      "  Using cached sse_starlette-1.8.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting stack-data==0.6.3 (from -r requirements.txt (line 412))\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting starlette==0.32.0.post1 (from -r requirements.txt (line 413))\n",
      "  Using cached starlette-0.32.0.post1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting starlette-context==0.3.6 (from -r requirements.txt (line 414))\n",
      "  Using cached starlette_context-0.3.6-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: sympy==1.12 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 415)) (1.12)\n",
      "Collecting tenacity==8.2.3 (from -r requirements.txt (line 416))\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: threadpoolctl==3.2.0 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 417)) (3.2.0)\n",
      "Collecting tiktoken==0.4.0 (from -r requirements.txt (line 418))\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-macosx_11_0_arm64.whl (761 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.4/761.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tinygrad==0.7.0 (from -r requirements.txt (line 419))\n",
      "  Using cached tinygrad-0.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting tokenizers==0.15.0 (from -r requirements.txt (line 420))\n",
      "  Using cached tokenizers-0.15.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tokentrim==0.1.13 (from -r requirements.txt (line 421))\n",
      "  Using cached tokentrim-0.1.13-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting toml==0.10.2 (from -r requirements.txt (line 422))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tomlkit==0.12.3 (from -r requirements.txt (line 423))\n",
      "  Using cached tomlkit-0.12.3-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting tools==0.1.9 (from -r requirements.txt (line 424))\n",
      "  Using cached tools-0.1.9.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 0.6.2 Requires-Python >=2.7.0.0,<2.8.0.0,>=3.4.0.0,<4.0.0.0; 0.6.3 Requires-Python >=2.7.0.0,<2.8.0.0,>=3.4.0.0,<4.0.0.0; 0.7 Requires-Python >=3.6, <3.7; 0.8 Requires-Python >=3.6, <3.7; 2.10.0 Requires-Python >=2.7,<3.0; 2.3.0 Requires-Python >=2.7,<3.0; 2.4.0 Requires-Python >=2.7,<3.0; 2.5.0 Requires-Python >=2.7,<3.0; 2.5.2 Requires-Python !=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,<3.9dev,>=2.7; 2.6.0 Requires-Python >=2.7,<3.0; 2.7.0 Requires-Python >=2.7,<3.0; 2.8.0 Requires-Python >=2.7,<3.0; 2.9.0 Requires-Python >=2.7,<3.0; 4.35 Requires-Python >=2.7, <3; 4.36 Requires-Python >=2.7, <3; 4.37 Requires-Python >=2.7, <3; 4.38 Requires-Python >=2.7, <3; 4.39 Requires-Python >=2.7, <3\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.3.0.dev20240101 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.3.0.dev20240101\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s2/Repos/dataclysm/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Resolving data files: 100%|██████████| 34/34 [00:00<00:00, 163.88it/s]\n",
      "/Users/s2/Repos/dataclysm/.conda/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msomewheresystems/dataclysm-arxiv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Convert the dataset to a pandas DataFrame\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Grab a random entry from the DataFrame\u001b[39;00m\n\u001b[1;32m     19\u001b[0m random_entry \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/Repos/dataclysm/.conda/lib/python3.10/site-packages/pandas/core/frame.py:798\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    796\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 798\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/Repos/dataclysm/.conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2384\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m   2383\u001b[0m             pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2384\u001b[0m             formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpa_subtable_ex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2386\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2387\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2388\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2389\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_all_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2391\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[1;32m   2392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Repos/dataclysm/.conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/Repos/dataclysm/.conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:396\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 396\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/Repos/dataclysm/.conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:436\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 436\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_row(row)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/Repos/dataclysm/.conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:144\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the huggingface dataset\n",
    "dataset = load_dataset('somewheresystems/dataclysm-arxiv')\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Grab a random entry from the DataFrame\n",
    "random_entry = df.sample()\n",
    "\n",
    "# Get the ID of the random entry\n",
    "id = random_entry['id'].values[0]\n",
    "\n",
    "# Construct the URL\n",
    "url = f\"https://arxiv.org/pdf/{id}.pdf\"\n",
    "\n",
    "# Download the PDF using wget\n",
    "wget.download(url, out=os.path.join(directory, f\"{id}.pdf\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install some more packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: FlagEmbedding in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (1.1.8)\n",
      "Requirement already satisfied: datasets in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from FlagEmbedding) (2.1.2)\n",
      "Collecting transformers==4.34.0 (from FlagEmbedding)\n",
      "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: accelerate>=0.20.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from FlagEmbedding) (0.26.1)\n",
      "Requirement already satisfied: sentence-transformers in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from FlagEmbedding) (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0->FlagEmbedding)\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from transformers==4.34.0->FlagEmbedding) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: psutil in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (5.9.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0->FlagEmbedding) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->FlagEmbedding) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->FlagEmbedding) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->FlagEmbedding) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->FlagEmbedding) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: torchvision in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (0.16.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (1.11.4)\n",
      "Requirement already satisfied: nltk in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sentence-transformers->FlagEmbedding) (0.1.99)\n",
      "Requirement already satisfied: six>=1.5 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tokenizers-0.14.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting sentence-transformers (from FlagEmbedding)\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "  Using cached sentence-transformers-2.2.1.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is still looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached sentence-transformers-2.0.0.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached sentence-transformers-1.2.1.tar.gz (80 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached sentence-transformers-1.2.0.tar.gz (81 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate>=0.20.1 (from FlagEmbedding)\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0->FlagEmbedding)\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.3)\n",
      "Requirement already satisfied: click in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from nltk->sentence-transformers->FlagEmbedding) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from nltk->sentence-transformers->FlagEmbedding) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->FlagEmbedding) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (from torchvision->sentence-transformers->FlagEmbedding) (10.2.0)\n",
      "Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "Using cached datasets-2.14.7-py3-none-any.whl (520 kB)\n",
      "Using cached tokenizers-0.14.1-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.2\n",
      "    Uninstalling huggingface-hub-0.20.2:\n",
      "      Successfully uninstalled huggingface-hub-0.20.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.16.1\n",
      "    Uninstalling datasets-2.16.1:\n",
      "      Successfully uninstalled datasets-2.16.1\n",
      "Successfully installed datasets-2.14.7 huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize arXiv Abstract + Title Indices\n",
    "This process takes ~15 minutes to index (M3 Max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Wikipedia Database + Index\n",
    "This process takes 2x as much time as arXiv to download, about ~12 minutes to index (M3 Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Current memory usage: 35964.890625 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 65/65 [00:00<00:00, 113.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage: 36104.484375 MB\n",
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text', 'title_embedding'],\n",
      "    num_rows: 6458670\n",
      "})\n",
      "['id', 'url', 'title', 'text', 'title_embedding']\n",
      "{'id': Value(dtype='int64', id=None), 'url': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'title_embedding': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None)}\n",
      "Current memory usage: 36104.484375 MB\n",
      "Flattening title_embedding and adding FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6458670/6458670 [12:42<00:00, 8470.21 examples/s]\n",
      "100%|██████████| 6459/6459 [00:39<00:00, 165.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index for title_embedding added.\n",
      "Current memory usage: 77425.046875 MB\n",
      "Current memory usage: 77425.234375 MB\n",
      "Datasets loaded.\n",
      "Initializing model...\n",
      "Model initialized.\n",
      "Current memory usage: 75355.0 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import FlagModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage():\n",
    "    print(f\"Current memory usage: {psutil.Process().memory_info().rss / 1024 ** 2} MB\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "print_memory_usage()\n",
    "dataclysm_wikipedia = load_dataset('somewheresystems/dataclysm-wikipedia', split=\"train\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Check the structure of the dataset, particularly the 'title_embedding' and 'abstract_embedding' columns\n",
    "print(dataclysm_wikipedia)\n",
    "print(dataclysm_wikipedia.column_names)\n",
    "print(dataclysm_wikipedia.features)\n",
    "print_memory_usage()\n",
    "\n",
    "# Define a function to flatten the embeddings and add FAISS index\n",
    "def flatten_and_add_faiss_index(dataset, column_name):\n",
    "    embedding_shape = np.array(dataset[0][column_name]).shape\n",
    "    if len(embedding_shape) == 2:\n",
    "        print(f\"Flattening {column_name} and adding FAISS index...\")\n",
    "        # Flatten the column before adding the FAISS index\n",
    "        dataset = dataset.map(lambda x: {column_name: np.concatenate(x[column_name])})\n",
    "        dataset = dataset.add_faiss_index(column=column_name)\n",
    "        print(f\"FAISS index for {column_name} added.\")\n",
    "    else:\n",
    "        print(f\"Cannot add FAISS index for {column_name}.\")\n",
    "    print_memory_usage()\n",
    "    return dataset\n",
    "\n",
    "# Add FAISS indices for 'title_embedding' and 'abstract_embedding' and save them to different datasets\n",
    "dataclysm_wikipedia_indexed = flatten_and_add_faiss_index(dataclysm_wikipedia, 'title_embedding')\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "# Define the model\n",
    "print(\"Initializing model...\")\n",
    "model = FlagModel('BAAI/bge-small-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"Write a representation of the following query which is optimized for using a similarity search for retrieval:\",\n",
    "                  use_fp16=True)\n",
    "print(\"Model initialized.\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize arXiv Abstract + Title Indices\n",
    "This process takes ~15 minutes to index (M3 Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Current memory usage: 71617.296875 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 34/34 [00:00<00:00, 127.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage: 71782.09375 MB\n",
      "Dataset({\n",
      "    features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'abstract', 'report-no', 'categories', 'versions', 'title_embedding', 'abstract_embedding'],\n",
      "    num_rows: 3360984\n",
      "})\n",
      "['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'abstract', 'report-no', 'categories', 'versions', 'title_embedding', 'abstract_embedding']\n",
      "{'id': Value(dtype='string', id=None), 'submitter': Value(dtype='string', id=None), 'authors': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'comments': Value(dtype='string', id=None), 'journal-ref': Value(dtype='string', id=None), 'doi': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None), 'report-no': Value(dtype='string', id=None), 'categories': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'versions': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'title_embedding': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None), 'abstract_embedding': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None)}\n",
      "Current memory usage: 71782.109375 MB\n",
      "Flattening title_embedding and adding FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3361/3361 [00:19<00:00, 172.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index for title_embedding added.\n",
      "Current memory usage: 81655.0 MB\n",
      "Flattening abstract_embedding and adding FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3360984/3360984 [08:51<00:00, 6318.74 examples/s]\n",
      "100%|██████████| 3361/3361 [00:20<00:00, 166.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index for abstract_embedding added.\n",
      "Current memory usage: 65721.453125 MB\n",
      "Current memory usage: 65720.3125 MB\n",
      "Datasets loaded.\n",
      "Initializing model...\n",
      "Model initialized.\n",
      "Current memory usage: 64288.28125 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import FlagModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage():\n",
    "    print(f\"Current memory usage: {psutil.Process().memory_info().rss / 1024 ** 2} MB\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "print_memory_usage()\n",
    "dataclysm_arxiv = load_dataset('somewheresystems/dataclysm-arxiv', split=\"train\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Check the structure of the dataset, particularly the 'title_embedding' and 'abstract_embedding' columns\n",
    "print(dataclysm_arxiv)\n",
    "print(dataclysm_arxiv.column_names)\n",
    "print(dataclysm_arxiv.features)\n",
    "print_memory_usage()\n",
    "\n",
    "# Define a function to flatten the embeddings and add FAISS index\n",
    "def flatten_and_add_faiss_index(dataset, column_name):\n",
    "    embedding_shape = np.array(dataset[0][column_name]).shape\n",
    "    if len(embedding_shape) == 2:\n",
    "        print(f\"Flattening {column_name} and adding FAISS index...\")\n",
    "        # Flatten the column before adding the FAISS index\n",
    "        dataset = dataset.map(lambda x: {column_name: np.concatenate(x[column_name])})\n",
    "        dataset = dataset.add_faiss_index(column=column_name)\n",
    "        print(f\"FAISS index for {column_name} added.\")\n",
    "    else:\n",
    "        print(f\"Cannot add FAISS index for {column_name}.\")\n",
    "    print_memory_usage()\n",
    "    return dataset\n",
    "\n",
    "# Add FAISS indices for 'title_embedding' and 'abstract_embedding' and save them to different datasets\n",
    "dataclysm_title_indexed = flatten_and_add_faiss_index(dataclysm_arxiv, 'title_embedding')\n",
    "dataclysm_abstract_indexed = flatten_and_add_faiss_index(dataclysm_arxiv, 'abstract_embedding')\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "# Define the model\n",
    "print(\"Initializing model...\")\n",
    "model = FlagModel('BAAI/bge-small-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"Write a representation of the following query which is optimized for using a similarity search for retrieval:\",\n",
    "                  use_fp16=True)\n",
    "print(\"Model initialized.\")\n",
    "print_memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  arXiv Composite Search with regex Rerank\n",
    "Search by both Abstract and Title similarity, rank both descending by score. \n",
    "1. If a duplicate (title and abstract hit) is found, it increases the score by a factor of 2. \n",
    "2. If regex finds the query in the abstract, it increases the score by 0.1 (additive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding query...\n",
      "Query encoded.\n",
      "Retrieving examples by abstract similarity...\n",
      "Examples retrieved.\n",
      "Retrieving examples by title similarity...\n",
      "Examples retrieved.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "QUERY: **Attention Is All You Need**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>versions</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>source</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>math/9409228</td>\n",
       "      <td>None</td>\n",
       "      <td>Alphonse P. Magnus</td>\n",
       "      <td>Painlev\\'e equations for semi-classical recurrence coefficients</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td><details><summary>Abstract</summary>  The title says it all.\\n</details></td>\n",
       "      <td>OP-SF 6 Sep 1994</td>\n",
       "      <td>[math.CA]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/math/9409228\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>math/9803101</td>\n",
       "      <td>Robion C. Kirby</td>\n",
       "      <td>Robion C. Kirby and Laurence R. Taylor</td>\n",
       "      <td>A survey of 4-manifolds through the eyes of surgery</td>\n",
       "      <td>25 pages. To appear in Wall's 60th birthday volume</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td><details><summary>Abstract</summary>  The title says it all.\\n</details></td>\n",
       "      <td>None</td>\n",
       "      <td>[math.GT]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/math/9803101\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1702.04226</td>\n",
       "      <td>Wenyun Ju</td>\n",
       "      <td>Wenyun Ju</td>\n",
       "      <td>Cascading Outage Simulation Based on Dynamic Fast Decoupled Load Flow\\n  Model</td>\n",
       "      <td>There is an error in equation (23)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td><details><summary>Abstract</summary>  Frequency is an important\\n</details></td>\n",
       "      <td>None</td>\n",
       "      <td>[cs.SY]</td>\n",
       "      <td>[v1, v2, v3]</td>\n",
       "      <td>0.979529</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1702.04226\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>math/9803061</td>\n",
       "      <td>Hans Schneider</td>\n",
       "      <td>Hans Schneider (U Wisconsin - Madison)</td>\n",
       "      <td>Some personal reminiscences of Olga Taussky</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td><details><summary>Abstract</summary>  The title says it all\\n</details></td>\n",
       "      <td>None</td>\n",
       "      <td>[math.HO math.RA]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>0.959123</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/math/9803061\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1806.06771</td>\n",
       "      <td>Christoph Anderson</td>\n",
       "      <td>Christoph Anderson, Isabel H\\\"ubener, Ann-Kathrin Seipp, Sandra Ohly,\\n  Klaus David, Veljko Pejovic</td>\n",
       "      <td>A Survey of Attention Management Systems in Ubiquitous Computing\\n  Environments</td>\n",
       "      <td>27 pages, 7 figures</td>\n",
       "      <td>Proceedings of the ACM on Interactive, Mobile, Wearable and\\n  Ubiquitous Technologies, vol. 2, no. 2, pp. 58:1-58:27, June 2018</td>\n",
       "      <td>10.1145/3214261</td>\n",
       "      <td><details><summary>Abstract</summary>  Today's information and communication devices provide always-on connectivity,\\ninstant access to an endless repository of information, and represent the most\\ndirect point of contact to almost any person in the world. Despite these\\nadvantages, devices such as smartphones or personal computers lead to the\\nphenomenon of attention fragmentation, continuously interrupting individuals'\\nactivities and tasks with notifications. Attention management systems aim to\\nprovide active support in such scenarios, managing interruptions, for example,\\nby postponing notifications to opportune moments for information delivery. In\\nthis article, we review attention management system research with a particular\\nfocus on ubiquitous computing environments. We first examine cognitive theories\\nof attention and extract guidelines for practical attention management systems.\\nMathematical models of human attention are at the core of these systems, and in\\nthis article, we review sensing and machine learning techniques that make such\\nmodels possible. We then discuss design challenges towards the implementation\\nof such systems, and finally, we investigate future directions in this area,\\npaving the way for new approaches and systems supporting users in their\\nattention management.\\n</details></td>\n",
       "      <td>None</td>\n",
       "      <td>[cs.HC]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>0.956954</td>\n",
       "      <td>A</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1806.06771\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1710.03743</td>\n",
       "      <td>Mat\\=iss Rikters</td>\n",
       "      <td>Mat\\=iss Rikters, Mark Fishel</td>\n",
       "      <td>Confidence through Attention</td>\n",
       "      <td>None</td>\n",
       "      <td>Machine Translation Summit XVI, Nagoya, Japan, September 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  Attention distributions of the generated translations are a useful bi-product\\nof attention-based recurrent neural network translation models and can be\\ntreated as soft alignments between the input and output tokens. In this work,\\nwe use attention distributions as a confidence metric for output translations.\\nWe present two strategies of using the attention distributions: filtering out\\nbad translations from a large back-translated corpus, and selecting the best\\ntranslation in a hybrid setup of two different translation systems. While\\nmanual evaluation indicated only a weak correlation between our confidence\\nscore and human judgments, the use-cases showed improvements of up to 2.22 BLEU\\npoints for filtering and 0.99 points for hybrid translation, tested on\\nEnglish<->German and English<->Latvian translation.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>0.735877</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1710.03743\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1810.10126</td>\n",
       "      <td>Yang Li</td>\n",
       "      <td>Yang Li, Lukasz Kaiser, Samy Bengio, Si Si</td>\n",
       "      <td>Area Attention</td>\n",
       "      <td>8 pages plus references</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  Existing attention mechanisms are trained to attend to individual items in a\\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\\ntoken or an image grid. We propose area attention: a way to attend to areas in\\nthe memory, where each area contains a group of items that are structurally\\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\\n1D memory such as natural language sentences. Importantly, the shape and the\\nsize of an area are dynamically determined via learning, which enables a model\\nto attend to information with varying granularity. Area attention can easily\\nwork with existing model architectures such as multi-head attention for\\nsimultaneously attending to multiple areas in the memory. We evaluate area\\nattention on two tasks: neural machine translation (both character and\\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\\nbaselines in all the cases. These improvements are obtainable with a basic form\\nof area attention that is parameter free.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.LG cs.AI cs.CL stat.ML]</td>\n",
       "      <td>[v1, v2, v3, v4, v5, v6]</td>\n",
       "      <td>0.710768</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1810.10126\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1810.13409</td>\n",
       "      <td>Ofir Press</td>\n",
       "      <td>Ofir Press, Noah A. Smith</td>\n",
       "      <td>You May Not Need Attention</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  In NMT, how far can we get without attention and without separate encoding\\nand decoding? To answer that question, we introduce a recurrent neural\\ntranslation model that does not use attention and does not have a separate\\nencoder and decoder. Our eager translation model is low-latency, writing target\\ntokens as soon as it reads the first source token, and uses constant memory\\nduring decoding. It performs on par with the standard attention-based model of\\nBahdanau et al. (2014), and better on long sentences.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>0.582988</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1810.13409\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1804.02391</td>\n",
       "      <td>Saumya Jetley</td>\n",
       "      <td>Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H.S. Torr</td>\n",
       "      <td>Learn To Pay Attention</td>\n",
       "      <td>International Conference on Learning Representations 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  We propose an end-to-end-trainable attention module for convolutional neural\\nnetwork (CNN) architectures built for image classification. The module takes as\\ninput the 2D feature vector maps which form the intermediate representations of\\nthe input image at different stages in the CNN pipeline, and outputs a 2D\\nmatrix of scores for each map. Standard CNN architectures are modified through\\nthe incorporation of this module, and trained under the constraint that a\\nconvex combination of the intermediate 2D feature vectors, as parameterised by\\nthe score matrices, must \\textit{alone} be used for classification.\\nIncentivised to amplify the relevant and suppress the irrelevant or misleading,\\nthe scores thus assume the role of attention values. Our experimental\\nobservations provide clear evidence to this effect: the learned attention maps\\nneatly highlight the regions of interest while suppressing background clutter.\\nConsequently, the proposed function is able to bootstrap standard CNN\\narchitectures for the task of image classification, demonstrating superior\\ngeneralisation over 6 unseen benchmark datasets. When binarised, our attention\\nmaps outperform other CNN-based attention maps, traditional saliency maps, and\\ntop object proposals for weakly supervised segmentation as demonstrated on the\\nObject Discovery dataset. We also demonstrate improved robustness against the\\nfast gradient sign method of adversarial attack.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CV cs.AI]</td>\n",
       "      <td>[v1, v2]</td>\n",
       "      <td>0.580535</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1804.02391\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1706.03762</td>\n",
       "      <td>Ashish Vaswani</td>\n",
       "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\n  Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>15 pages, 5 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL cs.LG]</td>\n",
       "      <td>[v1, v2, v3, v4, v5]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1706.03762\">Link</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Attention Is All You Need\"\n",
    "print(\"Encoding query...\")\n",
    "query_embedding = model.encode([query])\n",
    "print(\"Query encoded.\")\n",
    "\n",
    "print(\"Retrieving examples by abstract similarity...\")\n",
    "scores_abstract, retrieved_examples_abstract = dataclysm_abstract_indexed.get_nearest_examples('abstract_embedding', query_embedding, k=10)\n",
    "print(\"Examples retrieved.\")\n",
    "\n",
    "print(\"Retrieving examples by title similarity...\")\n",
    "scores_title, retrieved_examples_title = dataclysm_title_indexed.get_nearest_examples('title_embedding', query_embedding, k=10)\n",
    "print(\"Examples retrieved.\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Convert retrieved examples to DataFrame\n",
    "df_abstract = pd.DataFrame(retrieved_examples_abstract)\n",
    "df_title = pd.DataFrame(retrieved_examples_title)\n",
    "\n",
    "# Calculate similarity score in percentage\n",
    "df_abstract['similarity_score'] = scores_abstract\n",
    "df_title['similarity_score'] = scores_title\n",
    "\n",
    "# Add a column to denote the source of retrieval\n",
    "df_abstract['source'] = 'A'\n",
    "df_title['source'] = 'T'\n",
    "\n",
    "# Drop 'title_embedding' and 'abstract_embedding' columns\n",
    "df_abstract = df_abstract.drop(columns=['title_embedding', 'abstract_embedding'])\n",
    "df_title = df_title.drop(columns=['title_embedding', 'abstract_embedding'])\n",
    "\n",
    "# Drop empty columns\n",
    "df_abstract = df_abstract.dropna(axis=1, how='all')\n",
    "df_title = df_title.dropna(axis=1, how='all')\n",
    "\n",
    "# Create a \"click to expand\" for the abstract so it doesn't take up much space\n",
    "df_abstract['abstract'] = df_abstract['abstract'].apply(lambda x: f'<details><summary>Abstract</summary>{x}</details>')\n",
    "df_title['abstract'] = df_title['abstract'].apply(lambda x: f'<details><summary>Abstract</summary>{x}</details>')\n",
    "\n",
    "# Create a URL field with a hyperlink which is constructed by appending the id onto the end of arxiv.org/abs/\n",
    "df_abstract['URL'] = df_abstract['id'].apply(lambda x: f'<a href=\"https://arxiv.org/abs/{x}\">Link</a>')\n",
    "df_title['URL'] = df_title['id'].apply(lambda x: f'<a href=\"https://arxiv.org/abs/{x}\">Link</a>')\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df = pd.concat([df_abstract, df_title])\n",
    "\n",
    "# Normalize the similarity score to be between 0 and 1\n",
    "df['similarity_score'] = df['similarity_score'] / df['similarity_score'].max()\n",
    "\n",
    "# Increase the score if the query is found in the abstract\n",
    "df['similarity_score'] = df.apply(lambda row: row['similarity_score'] + 0.1 if re.search(query, row['abstract'], re.IGNORECASE) else row['similarity_score'], axis=1)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['id'])\n",
    "\n",
    "# Sort by ascending similarity score\n",
    "df = df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f'QUERY: **{query}**'))\n",
    "display(HTML(df.to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia simple search (Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Retrieval Augmented Generation\"\n",
    "print(\"Encoding query...\")\n",
    "query_embedding = model.encode([query])\n",
    "print(\"Query encoded.\")\n",
    "\n",
    "print(\"Retrieving examples by title similarity...\")\n",
    "scores, retrieved_examples = dataclysm_wikipedia_indexed.get_nearest_examples('title_embedding', query_embedding, k=10)\n",
    "print(\"Examples retrieved.\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Convert retrieved examples to DataFrame\n",
    "df = pd.DataFrame(retrieved_examples)\n",
    "\n",
    "# Calculate similarity score in percentage\n",
    "df['similarity_score'] = scores\n",
    "\n",
    "\n",
    "# Drop 'title_embedding' and 'abstract_embedding' columns\n",
    "df = df.drop(columns=['title_embedding'])\n",
    "\n",
    "# Drop empty columns\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Create a \"click to expand\" for the abstract so it doesn't take up much space\n",
    "df['text'] = df['text'].apply(lambda x: f'<details><summary>Article Text</summary>{x}</details>')\n",
    "\n",
    "\n",
    "# Create a URL field with a hyperlink \n",
    "df['url'] = df['url'].apply(lambda x: f'<a href=\"{url}\">Link</a>')\n",
    "\n",
    "# Sort by ascending similarity score\n",
    "df = df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f'QUERY: **{query}**'))\n",
    "display(HTML(df.to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download OpenHermes-2.5-Mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-cli in /Users/s2/Repos/harness/.conda/lib/python3.10/site-packages (0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf to /Users/s2/.cache/huggingface/hub/tmpdpa1h_vs\n",
      "openhermes-2.5-mistral-7b.Q4_K_M.gguf: 100%|█| 4.37G/4.37G [02:37<00:00, 27.8MB/\n",
      "./openhermes-2.5-mistral-7b.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface-cli\n",
    "!huggingface-cli download TheBloke/OpenHermes-2.5-Mistral-7B-GGUF openhermes-2.5-mistral-7b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from openhermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = teknium_openhermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1012.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   545.82 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is the title of a paper published in 2017 that introduced a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The paper achieved significant improvements in machine translation tasks and established a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6858.21 ms\n",
      "llama_print_timings:      sample time =       9.76 ms /    94 runs   (    0.10 ms per token,  9628.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   37015.65 ms /  2298 tokens (   16.11 ms per token,    62.08 tokens per second)\n",
      "llama_print_timings:        eval time =    5096.66 ms /    93 runs   (   54.80 ms per token,    18.25 tokens per second)\n",
      "llama_print_timings:       total time =   42232.10 ms /  2391 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp import LlamaGrammar\n",
    "import pandas as pd\n",
    "import json\n",
    "import httpx\n",
    "\n",
    "model = \"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\n",
    "prompt = f\"{df[['id', 'title', 'abstract']].to_html(escape=False)} ### Instruction: Use the information above to answer the query: EXPLAIN {query} ### Response:\"\n",
    "\n",
    "\n",
    "llm = Llama(model_path=model, n_ctx=8096, last_n_tokens_size=256, n_threads=4, n_gpu_layers=0)\n",
    "\n",
    "stream = llm.create_completion(prompt, stream=True, repeat_penalty=1.1, max_tokens=256, stop=[\"\\n\"], echo=False, temperature=0, mirostat_mode = 2, mirostat_tau=4.0, mirostat_eta=1.1)\n",
    "result = \"\"\n",
    "for output in stream:\n",
    "    result += output['choices'][0]['text']\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerank results using an LLM (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= arr \n",
      "arr ::= [[] [<U+000A>] ws arr_12 []] \n",
      "value ::= object | array | string | number | value_7 ws \n",
      "object ::= [{] ws object_16 [}] ws \n",
      "array ::= [[] ws array_20 []] ws \n",
      "string ::= [\"] string_23 [\"] ws \n",
      "number ::= number_24 number_30 number_34 ws \n",
      "value_7 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_36 \n",
      "arr_9 ::= value arr_11 \n",
      "arr_10 ::= [,] [<U+000A>] ws value \n",
      "arr_11 ::= arr_10 arr_11 | \n",
      "arr_12 ::= arr_9 | \n",
      "object_13 ::= string [:] ws value object_15 \n",
      "object_14 ::= [,] ws string [:] ws value \n",
      "object_15 ::= object_14 object_15 | \n",
      "object_16 ::= object_13 | \n",
      "array_17 ::= value array_19 \n",
      "array_18 ::= [,] ws value \n",
      "array_19 ::= array_18 array_19 | \n",
      "array_20 ::= array_17 | \n",
      "string_21 ::= [^\"\\] | [\\] string_22 \n",
      "string_22 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_23 ::= string_21 string_23 | \n",
      "number_24 ::= number_25 number_26 \n",
      "number_25 ::= [-] | \n",
      "number_26 ::= [0-9] | [1-9] number_27 \n",
      "number_27 ::= [0-9] number_27 | \n",
      "number_28 ::= [.] number_29 \n",
      "number_29 ::= [0-9] number_29 | [0-9] \n",
      "number_30 ::= number_28 | \n",
      "number_31 ::= [eE] number_32 number_33 \n",
      "number_32 ::= [-+] | \n",
      "number_33 ::= [0-9] number_33 | [0-9] \n",
      "number_34 ::= number_31 | \n",
      "ws_35 ::= [ <U+0009><U+000A>] ws \n",
      "ws_36 ::= ws_35 | \n",
      "\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from openhermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = teknium_openhermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1012.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   545.82 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n \"1706.03762\",\\n \"1804.02391\",\\n \"1810.13409\",\\n \"1810.10126\"\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6855.33 ms\n",
      "llama_print_timings:      sample time =     271.12 ms /    55 runs   (    4.93 ms per token,   202.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38534.66 ms /  2402 tokens (   16.04 ms per token,    62.33 tokens per second)\n",
      "llama_print_timings:        eval time =    2956.72 ms /    54 runs   (   54.75 ms per token,    18.26 tokens per second)\n",
      "/var/folders/jq/w3ddfwz910xdn6p611vx4sjh0000gn/T/ipykernel_91434/3247289090.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['id'] = pd.Categorical(filtered_df['id'], categories=result_ids, ordered=True)\n",
      "llama_print_timings:       total time =   41870.97 ms /  2456 tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>versions</th>\n",
       "      <th>source</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1706.03762</td>\n",
       "      <td>Ashish Vaswani</td>\n",
       "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\n  Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>15 pages, 5 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL cs.LG]</td>\n",
       "      <td>[v1, v2, v3, v4, v5]</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1706.03762\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1804.02391</td>\n",
       "      <td>Saumya Jetley</td>\n",
       "      <td>Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H.S. Torr</td>\n",
       "      <td>Learn To Pay Attention</td>\n",
       "      <td>International Conference on Learning Representations 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  We propose an end-to-end-trainable attention module for convolutional neural\\nnetwork (CNN) architectures built for image classification. The module takes as\\ninput the 2D feature vector maps which form the intermediate representations of\\nthe input image at different stages in the CNN pipeline, and outputs a 2D\\nmatrix of scores for each map. Standard CNN architectures are modified through\\nthe incorporation of this module, and trained under the constraint that a\\nconvex combination of the intermediate 2D feature vectors, as parameterised by\\nthe score matrices, must \\textit{alone} be used for classification.\\nIncentivised to amplify the relevant and suppress the irrelevant or misleading,\\nthe scores thus assume the role of attention values. Our experimental\\nobservations provide clear evidence to this effect: the learned attention maps\\nneatly highlight the regions of interest while suppressing background clutter.\\nConsequently, the proposed function is able to bootstrap standard CNN\\narchitectures for the task of image classification, demonstrating superior\\ngeneralisation over 6 unseen benchmark datasets. When binarised, our attention\\nmaps outperform other CNN-based attention maps, traditional saliency maps, and\\ntop object proposals for weakly supervised segmentation as demonstrated on the\\nObject Discovery dataset. We also demonstrate improved robustness against the\\nfast gradient sign method of adversarial attack.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CV cs.AI]</td>\n",
       "      <td>[v1, v2]</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1804.02391\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1810.13409</td>\n",
       "      <td>Ofir Press</td>\n",
       "      <td>Ofir Press, Noah A. Smith</td>\n",
       "      <td>You May Not Need Attention</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  In NMT, how far can we get without attention and without separate encoding\\nand decoding? To answer that question, we introduce a recurrent neural\\ntranslation model that does not use attention and does not have a separate\\nencoder and decoder. Our eager translation model is low-latency, writing target\\ntokens as soon as it reads the first source token, and uses constant memory\\nduring decoding. It performs on par with the standard attention-based model of\\nBahdanau et al. (2014), and better on long sentences.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>[v1]</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1810.13409\">Link</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1810.10126</td>\n",
       "      <td>Yang Li</td>\n",
       "      <td>Yang Li, Lukasz Kaiser, Samy Bengio, Si Si</td>\n",
       "      <td>Area Attention</td>\n",
       "      <td>8 pages plus references</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td><details><summary>Abstract</summary>  Existing attention mechanisms are trained to attend to individual items in a\\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\\ntoken or an image grid. We propose area attention: a way to attend to areas in\\nthe memory, where each area contains a group of items that are structurally\\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\\n1D memory such as natural language sentences. Importantly, the shape and the\\nsize of an area are dynamically determined via learning, which enables a model\\nto attend to information with varying granularity. Area attention can easily\\nwork with existing model architectures such as multi-head attention for\\nsimultaneously attending to multiple areas in the memory. We evaluate area\\nattention on two tasks: neural machine translation (both character and\\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\\nbaselines in all the cases. These improvements are obtainable with a basic form\\nof area attention that is parameter free.\\n</details></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cs.LG cs.AI cs.CL stat.ML]</td>\n",
       "      <td>[v1, v2, v3, v4, v5, v6]</td>\n",
       "      <td>T</td>\n",
       "      <td><a href=\"https://arxiv.org/abs/1810.10126\">Link</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp import LlamaGrammar\n",
    "import pandas as pd\n",
    "import json\n",
    "import httpx\n",
    "grammar_text = httpx.get(\"https://raw.githubusercontent.com/ggerganov/llama.cpp/master/grammars/json_arr.gbnf\").text\n",
    "grammar = LlamaGrammar.from_string(grammar_text)\n",
    "\n",
    "model = \"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\n",
    "prompt = f\"\"\"You are an expert at generating valid JSON.\n",
    "###\n",
    "Instruction:\n",
    "Return a valid JSON Array containing arXiv ['id'] field reranked according to how relevant the result is to the query based on its other columns at that ['id']. Drop any items that are not relevant to the query. Return just an array of the IDs, like [x,y,z] and so on in the correct order:\n",
    "        INDEX: {df[['id', 'title', 'abstract']].to_html(escape=False)}\n",
    "        QUERY: {query}\n",
    "        Take a deep breath, and solve the problem step-by-step.\n",
    "###\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "llm = Llama(model_path=model, n_ctx=8096, last_n_tokens_size=256, n_threads=4, n_gpu_layers=0)\n",
    "\n",
    "    \n",
    "stream = llm.create_completion(prompt, stream=True, repeat_penalty=1.1, max_tokens=256, stop=[\"]\"], echo=False, temperature=0, mirostat_mode = 2, mirostat_tau=4.0, mirostat_eta=1.1, grammar=grammar)\n",
    "result = \"\"\n",
    "for output in stream:\n",
    "    result += output['choices'][0]['text']\n",
    "\n",
    "result = result + \"]\"\n",
    "\n",
    "# Check if the result is a string, an array string, or a single ID in an array and convert it to a list of IDs\n",
    "if isinstance(result, str):\n",
    "    result_ids = [result.strip('[]')]\n",
    "elif isinstance(result, list):\n",
    "    if isinstance(result[0], str):\n",
    "        result_ids = [json.loads(res) for res in result]\n",
    "    else:\n",
    "        result_ids = result\n",
    "# Print the result\n",
    "print(result_ids)\n",
    "import re\n",
    "\n",
    "# Extract IDs from the potentially broken string using regex\n",
    "result_ids = re.findall(r'\"(.*?)\"', result_ids[0])\n",
    "\n",
    "# Filter the dataframe to only include rows with IDs in the result\n",
    "filtered_df = df[df['id'].isin(result_ids)]\n",
    "\n",
    "# Create a categorical type for sorting based on the order in result_ids\n",
    "filtered_df['id'] = pd.Categorical(filtered_df['id'], categories=result_ids, ordered=True)\n",
    "\n",
    "# Sort the dataframe based on the 'id' column\n",
    "filtered_df = filtered_df.sort_values('id')\n",
    "\n",
    "# Drop the similarity score column\n",
    "filtered_df = filtered_df.drop(columns=['similarity_score'])\n",
    "\n",
    "# Display the filtered dataframe as a table with hyperlinks\n",
    "display(HTML(filtered_df.to_html(escape=False)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
